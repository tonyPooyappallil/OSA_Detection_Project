{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyPooyappallil/OSA_Detection_Project/blob/main/signal_window_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lHwbI82iEv7",
        "outputId": "532d73d6-52cf-498e-c7bb-a611499d104e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# PATH to the uploaded ZIP file in Google drive\n",
        "zip_file_path = '/content/drive/MyDrive/AI_Sleep_Apnea/data/selected_records208.zip'\n",
        "\n",
        "#zip_file_path = '/content/drive/MyDrive/AI_Sleep_Apnea/data/selected_records_stratified_30.zip'\n",
        "# folder name inside the zip\n",
        "extracted_folder_name = 'selected_records208'\n",
        "#extracted_folder_name = 'selected_records_stratified_30'\n",
        "\n",
        "# Defining where to unzip inside Colab's temporary storage\n",
        "# Since Using /content/ makes it faster to access during processing\n",
        "unzip_dir = '/content/temp_data'\n",
        "\n",
        "# --- Unzipping command ---\n",
        "print(f\"Creating directory {unzip_dir} if it doesn't exist...\")\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping {zip_file_path} to {unzip_dir}...\")\n",
        "# Using '-o' to overwrite without asking if run again, '-q' for quiet\n",
        "!unzip -oq \"{zip_file_path}\" -d \"{unzip_dir}\"\n",
        "\n",
        "print(\"Unzipping complete.\")\n",
        "\n",
        "# --- Defining the path to the actual data *after* the unzipping ---\n",
        "# This is the path that will be used in the main script's config\n",
        "colab_data_path = os.path.join(unzip_dir, extracted_folder_name)\n",
        "print(f\"Data should now be available in: {colab_data_path}\")\n",
        "\n",
        "# Verifying some contents\n",
        "print(\"Example content listing:\")\n",
        "!ls \"{colab_data_path}\" | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icVBmB6ziWs_",
        "outputId": "98008eb6-7595-45c6-9afa-ea3cd21038d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating directory /content/temp_data if it doesn't exist...\n",
            "Unzipping /content/drive/MyDrive/AI_Sleep_Apnea/data/selected_records208.zip to /content/temp_data...\n",
            "Unzipping complete.\n",
            "Data should now be available in: /content/temp_data/selected_records208\n",
            "Example content listing:\n",
            "ls: cannot access '/content/temp_data/selected_records208': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing Dependencies\n",
        "!pip install antropy scikit-image h5py wfdb -q\n",
        "print(\"Dependencies installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwxzKTfXIf0n",
        "outputId": "deb2980b-9b2b-4ccf-ffb5-b3238d93c7a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "import scipy.signal\n",
        "import h5py # For the arousal .mat files v7.3\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import RandomForestClassifier # For the secondary model\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "import time\n",
        "import gc\n",
        "import warnings\n",
        "import wfdb\n",
        "import skimage.transform\n",
        "# import zipfile # Zipping logic now removed, import can be removed if not re-added\n",
        "import copy # For deep copying of config\n",
        "import multiprocessing\n",
        "\n",
        "# Trying to import antropy, skimage, h5py needed for features/resizing/loading\n",
        "try: import antropy\n",
        "except ImportError: print(\"Error: pip install antropy\"); exit()\n",
        "try: import skimage.transform\n",
        "except ImportError: print(\"Error: pip install scikit-image\"); exit()\n",
        "try: import h5py\n",
        "except ImportError: print(\"Error: pip install h5py\"); exit()\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/AI_Sleep_Apnea/' # DRIVE PATH\n",
        "UNZIPPED_DATA_PARENT_DIR = '/content/temp_data' # Local temp directory for the unzipped data\n",
        "DATA_FOLDER_NAME = 'selected_records208' # Using the 208 records\n",
        "DATA_DIR = os.path.join(UNZIPPED_DATA_PARENT_DIR, DATA_FOLDER_NAME)\n",
        "LABELS_FILE = os.path.join(DATA_DIR, \"selected_records_ahi_results_with_severity.csv\") # Path to AHI results ground truth\n",
        "DEMOGRAPHICS_FILE = os.path.join(DATA_DIR, \"age-sex.csv\") # Path to the demographics file\n",
        "AROUSAL_MAT_SUFFIX = \"-arousal.mat\" # Suffix for the arousal files (used by wfdb.rdann)\n",
        "\n",
        "# Output Directories\n",
        "PERSISTENT_OUTPUT_DIR_BASE = os.path.join(DRIVE_PROJECT_PATH, \"output/sleep_apnea_analysis_simplified_208_v2_secondary_model\") # New output folder\n",
        "LOCAL_DATA_DIR_BASE = \"/content/processed_data_local_simplified_v2\" # Local temp for the processed window data\n",
        "FINAL_MODEL_RESULTS_DIR = os.path.join(PERSISTENT_OUTPUT_DIR_BASE, \"final_model_evaluation_results\")\n",
        "\n",
        "# Processed Data Storage\n",
        "PERSISTENT_METADATA_CSV_NAME = \"all_processed_windows_metadata.csv\"\n",
        "# PERSISTENT_WINDOW_DATA_ZIP_NAME = \"all_processed_windows_data.zip\" # Zipping removed\n",
        "\n",
        "# Signal Processing Parameters\n",
        "SAMPLING_RATE = 200\n",
        "WINDOW_SECONDS = 30\n",
        "WINDOW_SAMPLES = int(WINDOW_SECONDS * SAMPLING_RATE)\n",
        "WINDOW_OVERLAP_RATIO = 0.5\n",
        "WINDOW_STEP_SAMPLES = int(WINDOW_SAMPLES * (1 - WINDOW_OVERLAP_RATIO))\n",
        "if WINDOW_STEP_SAMPLES <= 0: WINDOW_STEP_SAMPLES = WINDOW_SAMPLES # Ensuring step is at least 1 window\n",
        "\n",
        "SPECTROGRAM_SIGNALS = [\"SaO2\", \"ECG\"] # Signals to be converted to spectrograms\n",
        "RAW_TS_SIGNALS = ['AIRFLOW', 'C3-M2'] # Signals to be used as raw time series\n",
        "BASE_TABULAR_FEATURES = ['Age', 'Sex_encoded'] # Tabular features from demographics data\n",
        "\n",
        "SPECTROGRAM_NPERSEG = 256\n",
        "SPECTROGRAM_NOVERLAP = 128\n",
        "SPECTROGRAM_TARGET_SIZE = (64, 128) # (height, width) for the spectrogram images\n",
        "\n",
        "# Model Training Parameters\n",
        "RANDOM_STATE = 42\n",
        "BATCH_SIZE = 32\n",
        "GLOBAL_INITIAL_EPOCHS = 15 # Epochs for the initial training phase\n",
        "GLOBAL_FINE_TUNE_EPOCHS = 15 # Epochs for the fine-tuning phase\n",
        "\n",
        "# Default Hyperparameters (used directly as K-Fold is now removed)\n",
        "DEFAULT_LEARNING_RATE = 1e-4\n",
        "DEFAULT_FINE_TUNE_LR_FACTOR = 0.1 # Factor to reduce LR for the fine-tuning\n",
        "DEFAULT_FINE_TUNE_LAYERS = 15 # Number of layers to unfreeze in MobileNetV2 for fine-tuning\n",
        "DEFAULT_FOCAL_LOSS_GAMMA = 3.0 # Using increased gamma\n",
        "DEFAULT_L2_REG_FACTOR = 1e-5\n",
        "DEFAULT_ENABLE_OVERSAMPLING = True # For window-level labels during training\n",
        "DEFAULT_OVERSAMPLING_TARGET_POSITIVE_RATIO = 0.3\n",
        "DEFAULT_AUGMENT_SPECTROGRAMS = True\n",
        "DEFAULT_AUGMENT_RAW_TS = True\n",
        "\n",
        "# Workflow Control\n",
        "MAX_RECORDS_TO_PROCESS = 208 # Process all of the 208 records\n",
        "FORCE_REPROCESS_ALL_WINDOWS = False # If True, re-generates all window data. If False, tries to load the existing.\n",
        "FINAL_HOLDOUT_TEST_SET_RATIO = 0.2 # Proportion of the data for the final test set\n",
        "# Proportion of the train+val pool for validation set, used for threshold tuning\n",
        "FINAL_MODEL_VALIDATION_RATIO = 0.15 / (1.0 - FINAL_HOLDOUT_TEST_SET_RATIO) if (1.0 - FINAL_HOLDOUT_TEST_SET_RATIO) > 0 else 0.15\n",
        "USE_SECONDARY_PATIENT_MODEL = True # Flag to use the secondary model approach\n",
        "\n",
        "# Multi-Class Severity Configuration\n",
        "PATIENT_SEVERITY_TARGET_COL = 'AHI_Severity_MultiClass' # Column name in patient_true_labels_df\n",
        "PATIENT_BINARY_TARGET_COL = 'OSA_Severity_Binary' # Column name for binary OSA (AHI >= 5)\n",
        "# Default thresholds for mapping the positive window ratio to patient severity (0:Normal, 1:Mild, 2:Moderate, 3:Severe)\n",
        "# These will be tuned on the validation set if USE_SECONDARY_PATIENT_MODEL is False.\n",
        "PATIENT_SEVERITY_THRESHOLDS_ON_RATIO = [0.04, 0.10, 0.20] # Initial guess\n",
        "\n",
        "\n",
        "# --- Function to Save the Confusion Matrix Plot ---\n",
        "def save_confusion_matrix_plot(cm, class_names, title, filename):\n",
        "    \"\"\"\n",
        "    Saves a confusion matrix plot using seaborn and matplotlib.\n",
        "    Args:\n",
        "        cm (numpy.array): The confusion matrix.\n",
        "        class_names (list): List of class names for labels.\n",
        "        title (str): Title for the plot.\n",
        "        filename (str): Full path to save the plot image.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        os.makedirs(os.path.dirname(filename), exist_ok=True) # Ensure directory exists\n",
        "        plt.savefig(filename)\n",
        "        plt.close() # Close the figure to free memory\n",
        "        print(f\"Saved confusion matrix plot to: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in save_confusion_matrix_plot for '{title}': {e}\")\n",
        "\n",
        "# --- Helper Functions (Data Loading, Processing) ---\n",
        "def prepare_output_dirs_windowed(persistent_base_dir, local_base_dir_config_val):\n",
        "    print(f\"Ensuring persistent base output directory exists: {persistent_base_dir}\")\n",
        "    os.makedirs(persistent_base_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Preparing local base directory for processing/training: {local_base_dir_config_val}\")\n",
        "    if os.path.exists(local_base_dir_config_val):\n",
        "        print(f\"  Removing existing local directory: {local_base_dir_config_val}\")\n",
        "        shutil.rmtree(local_base_dir_config_val)\n",
        "    # Create the structure for storing processed window data locally\n",
        "    # This is where .npy files will be saved.\n",
        "    os.makedirs(os.path.join(local_base_dir_config_val, \"all_data_temp\", \"raw_ts\"), exist_ok=True)\n",
        "    for signal_name in SPECTROGRAM_SIGNALS:\n",
        "        os.makedirs(os.path.join(local_base_dir_config_val, \"all_data_temp\", \"spectrograms\", signal_name), exist_ok=True)\n",
        "\n",
        "    os.makedirs(os.path.join(persistent_base_dir, \"final_model_evaluation_results\"), exist_ok=True)\n",
        "    print(\"Output directories prepared.\")\n",
        "\n",
        "def load_labels_and_metadata(labels_file, demo_file,\n",
        "                             patient_binary_target_col='OSA_Severity_Binary',\n",
        "                             patient_multiclass_target_col='AHI_Severity_MultiClass',\n",
        "                             ahi_threshold_binary=5,\n",
        "                             ahi_thresholds_multiclass=[5, 15, 30]):\n",
        "    labels_df = pd.DataFrame()\n",
        "    demo_dict = {}\n",
        "    try:\n",
        "        labels_df = pd.read_csv(labels_file)\n",
        "        if 'Record' in labels_df.columns: labels_df['Record'] = labels_df['Record'].astype(str)\n",
        "        else: print(f\"Warning: 'Record' column not found in labels file: {labels_file}\"); labels_df['Record'] = None\n",
        "\n",
        "        if 'AHI' in labels_df.columns:\n",
        "            labels_df[patient_binary_target_col] = (labels_df['AHI'] >= ahi_threshold_binary).astype(int)\n",
        "            conditions = [\n",
        "                (labels_df['AHI'] < ahi_thresholds_multiclass[0]),\n",
        "                (labels_df['AHI'] >= ahi_thresholds_multiclass[0]) & (labels_df['AHI'] < ahi_thresholds_multiclass[1]),\n",
        "                (labels_df['AHI'] >= ahi_thresholds_multiclass[1]) & (labels_df['AHI'] < ahi_thresholds_multiclass[2]),\n",
        "                (labels_df['AHI'] >= ahi_thresholds_multiclass[2])\n",
        "            ]\n",
        "            choices = [0, 1, 2, 3]\n",
        "            labels_df[patient_multiclass_target_col] = np.select(conditions, choices, default=0)\n",
        "            print(f\"Created patient multi-class target '{patient_multiclass_target_col}' using AHI thresholds: <{ahi_thresholds_multiclass[0]} (0), \"\n",
        "                  f\"[{ahi_thresholds_multiclass[0]}-{ahi_thresholds_multiclass[1]}) (1), \"\n",
        "                  f\"[{ahi_thresholds_multiclass[1]}-{ahi_thresholds_multiclass[2]}) (2), \"\n",
        "                  f\">={ahi_thresholds_multiclass[2]} (3).\")\n",
        "            print(f\"Created patient binary target '{patient_binary_target_col}' using AHI threshold: >={ahi_threshold_binary} (1).\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: 'AHI' column not found in labels file. Cannot create patient targets '{patient_binary_target_col}' or '{patient_multiclass_target_col}'.\")\n",
        "            labels_df[patient_binary_target_col] = pd.NA\n",
        "            labels_df[patient_multiclass_target_col] = pd.NA\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Labels file not found at: {labels_file}. Patient-level true labels will be unavailable.\")\n",
        "        labels_df = pd.DataFrame(columns=['Record', patient_binary_target_col, patient_multiclass_target_col])\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading labels file or creating patient target: {e}\")\n",
        "        labels_df = pd.DataFrame(columns=['Record', patient_binary_target_col, patient_multiclass_target_col])\n",
        "\n",
        "    try:\n",
        "        demo_df = pd.read_csv(demo_file)\n",
        "        if 'Record' in demo_df.columns: demo_df['Record'] = demo_df['Record'].astype(str)\n",
        "        if 'Sex' in demo_df.columns: demo_df['Sex_encoded'] = LabelEncoder().fit_transform(demo_df['Sex'])\n",
        "        else: demo_df['Sex_encoded'] = np.nan\n",
        "\n",
        "        if 'Record' in demo_df.columns: demo_dict = demo_df.set_index('Record').to_dict('index')\n",
        "        else: print(f\"Warning: 'Record' column not found in demographics file: {demo_file}\")\n",
        "    except FileNotFoundError: print(f\"Warning: Demographics file not found at: {demo_file}\")\n",
        "    except Exception as e: print(f\"Error loading demographics file: {e}\")\n",
        "\n",
        "    return labels_df, demo_dict\n",
        "\n",
        "def read_signal_names(hea_path):\n",
        "    try:\n",
        "        with open(hea_path, 'r') as f: lines = f.readlines()\n",
        "        return [line.split(' ')[-1].strip() for line in lines[1:] if len(line.split(' ')) > 7]\n",
        "    except Exception as e: print(f\"Error reading signal names from {hea_path}: {e}\"); return None\n",
        "\n",
        "def load_signal_data(mat_path):\n",
        "    try:\n",
        "        with h5py.File(mat_path, 'r') as f:\n",
        "            data_key = 'val'; found=False\n",
        "            if data_key in f and isinstance(f[data_key], h5py.Dataset): found=True\n",
        "            else:\n",
        "                for k_search in ['signal_data', 'data', 'signals', 'val']:\n",
        "                    if k_search in f and isinstance(f[k_search], h5py.Dataset): data_key=k_search; found=True; break\n",
        "            if not found:\n",
        "                largest_size = 0\n",
        "                for k_search in f.keys():\n",
        "                    if isinstance(f[k_search], h5py.Dataset) and f[k_search].size > largest_size:\n",
        "                         largest_size = f[k_search].size; data_key = k_search; found=True\n",
        "            if not found: print(f\"Error: Cannot find signal data key in HDF5 file {mat_path}\"); return None\n",
        "            data = f[data_key][()].astype(np.float32)\n",
        "            return data\n",
        "    except OSError:\n",
        "        try:\n",
        "            mat = scipy.io.loadmat(mat_path); data_key = 'val'\n",
        "            if data_key not in mat:\n",
        "                for k_search,v_search in mat.items():\n",
        "                    if k_search.startswith('__'): continue\n",
        "                    if isinstance(v_search, np.ndarray) and v_search.ndim > 1 : data_key=k_search; break\n",
        "            if data_key not in mat or mat[data_key] is None : print(f\"Error: No signal data in MAT file {mat_path} (scipy)\"); return None\n",
        "            return mat[data_key].astype(np.float32)\n",
        "        except Exception as e_scipy: print(f\"Error loading signal data {mat_path} with scipy: {e_scipy}\"); return None\n",
        "    except Exception as e: print(f\"Error loading signal data {mat_path}: {e}\"); return None\n",
        "\n",
        "def load_apnea_hypopnea_events_wfdb(record_base_path, event_patterns_re, annotation_extension='arousal'):\n",
        "    events = []\n",
        "    try:\n",
        "        annotation = wfdb.rdann(record_base_path, extension=annotation_extension)\n",
        "        n_ann = len(annotation.sample)\n",
        "        i = 0\n",
        "        while i < n_ann:\n",
        "            note = annotation.aux_note[i].strip()\n",
        "            if note.startswith('('):\n",
        "                tag_content = note[1:]\n",
        "                matched_pattern = None\n",
        "                for pattern in event_patterns_re:\n",
        "                    if pattern.fullmatch(tag_content): matched_pattern = pattern; break\n",
        "                if matched_pattern:\n",
        "                    start_sample = annotation.sample[i]\n",
        "                    expected_end_tag = tag_content + ')'\n",
        "                    found_end = False\n",
        "                    for j in range(i + 1, n_ann):\n",
        "                        end_note = annotation.aux_note[j].strip()\n",
        "                        if end_note == expected_end_tag:\n",
        "                            end_sample = annotation.sample[j]\n",
        "                            if start_sample < end_sample: events.append((start_sample, end_sample))\n",
        "                            else: print(f\"Warning: End tag '{expected_end_tag}' before start in {record_base_path} at {end_sample} (start {start_sample})\")\n",
        "                            i = j; found_end = True; break\n",
        "            i += 1\n",
        "        return events\n",
        "    except FileNotFoundError: print(f\"Warning: Annotation file {record_base_path}.{annotation_extension} not found.\"); return None\n",
        "    except Exception as e: print(f\"Error reading WFDB annotations for {record_base_path}: {e}\"); return None\n",
        "\n",
        "def generate_and_save_numerical_spectrogram(signal_window, sr, output_path_npy, nperseg, noverlap, target_img_size_hw):\n",
        "    try:\n",
        "        if len(signal_window) < nperseg:\n",
        "            padding = nperseg - len(signal_window)\n",
        "            signal_window = np.pad(signal_window, (0, padding), 'constant', constant_values=0.0)\n",
        "\n",
        "        f, t, Sxx = scipy.signal.spectrogram(signal_window, fs=sr, nperseg=nperseg, noverlap=noverlap)\n",
        "\n",
        "        if Sxx.size == 0:\n",
        "            resized_Sxx_db = np.zeros(target_img_size_hw, dtype=np.float32)\n",
        "        else:\n",
        "            Sxx_db = 10 * np.log10(np.maximum(Sxx, 1e-9))\n",
        "            resized_Sxx_db = skimage.transform.resize(Sxx_db, target_img_size_hw, anti_aliasing=True, preserve_range=True).astype(np.float32)\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_path_npy), exist_ok=True)\n",
        "        np.save(output_path_npy, resized_Sxx_db)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        if isinstance(e, OSError) and e.errno == 28:\n",
        "            print(f\"FATAL ERROR: No space left on device saving: {output_path_npy}\");\n",
        "            raise e\n",
        "        print(f\"Error generating window spectrogram {output_path_npy}: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_numerical_spectrogram(path_npy, target_size_hw, augment=False, config_params=None):\n",
        "    try:\n",
        "        spec_array_2d = np.load(path_npy)\n",
        "        if spec_array_2d.shape != target_size_hw:\n",
        "            spec_array_2d = skimage.transform.resize(spec_array_2d, target_size_hw, anti_aliasing=True, preserve_range=True).astype(np.float32)\n",
        "\n",
        "        if augment and config_params and config_params.get('AUGMENT_SPECTROGRAMS', False):\n",
        "            if random.random() < 0.3:\n",
        "                h, w = spec_array_2d.shape\n",
        "                cutout_h = random.randint(max(1, h // 10), max(2, h // 4))\n",
        "                cutout_w = random.randint(max(1, w // 10), max(2, w // 4))\n",
        "                if h > cutout_h and w > cutout_w :\n",
        "                    y_start = random.randint(0, h - cutout_h); x_start = random.randint(0, w - cutout_w)\n",
        "                    spec_array_2d[y_start:y_start+cutout_h, x_start:x_start+cutout_w] = np.mean(spec_array_2d)\n",
        "            if random.random() < 0.2:\n",
        "                factor = random.uniform(0.7, 1.3)\n",
        "                spec_array_2d = spec_array_2d * factor\n",
        "                spec_array_2d = np.clip(spec_array_2d, np.min(spec_array_2d), np.max(spec_array_2d))\n",
        "\n",
        "        spec_array_3d_single = np.expand_dims(spec_array_2d, axis=-1)\n",
        "        spec_array_3d_rgb = np.concatenate([spec_array_3d_single] * 3, axis=-1)\n",
        "        return spec_array_3d_rgb.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading spec {path_npy}: {e}\")\n",
        "        # Return a NumPy array of zeros with correct shape\n",
        "        return np.zeros(target_size_hw + (3,), dtype=np.float32)\n",
        "\n",
        "def load_npy(path, expected_shape=None, augment=False, config_params=None):\n",
        "    try:\n",
        "        arr = np.load(path).astype(np.float32)\n",
        "        if expected_shape and arr.shape != expected_shape:\n",
        "            print(f\"Warning: Shape mismatch loading {path}. Expected {expected_shape}, got {arr.shape}.\")\n",
        "            return None\n",
        "        if augment and config_params and config_params.get('AUGMENT_RAW_TS', False) and arr.ndim == 1:\n",
        "            if random.random() < 0.3:\n",
        "                noise_factor = 0.005 * np.std(arr) if np.std(arr) > 1e-6 else 0.005\n",
        "                if noise_factor > 0 : arr = arr + np.random.normal(0, noise_factor, arr.shape)\n",
        "            if random.random() < 0.2:\n",
        "                scale_factor = random.uniform(0.9, 1.1); arr = arr * scale_factor\n",
        "        return arr\n",
        "    except Exception as e: print(f\"Error loading npy {path}: {e}\"); return None\n",
        "\n",
        "def process_record_windowed_mp_wrapper(args):\n",
        "    record_id_str, split_name_for_saving, config_dict_mp, demo_meta_record_mp, base_save_path_mp = args\n",
        "    return process_record_windowed(record_id_str, split_name_for_saving, config_dict_mp, demo_meta_record_mp, base_save_path_mp)\n",
        "\n",
        "def process_record_windowed(record_id, split_name_for_saving, config_dict, demo_meta_record, base_save_path):\n",
        "    # print(f\"PROCESS_RECORD_WINDOWED: Start processing record_id: {record_id}, PID: {os.getpid()}\") # Debug log (optional)\n",
        "\n",
        "    record_dir = os.path.join(config_dict['data_dir'], record_id)\n",
        "    mat_path = os.path.join(record_dir, f\"{record_id}.mat\")\n",
        "    hea_path = os.path.join(record_dir, f\"{record_id}.hea\")\n",
        "    record_base_path = os.path.join(record_dir, record_id)\n",
        "    processed_windows_metadata = []\n",
        "\n",
        "    if not (os.path.exists(mat_path) and os.path.exists(hea_path)):\n",
        "        print(f\"Warning: MAT or HEA file missing for record {record_id}. Skipping. MAT: {mat_path}, HEA: {hea_path}\")\n",
        "        return []\n",
        "\n",
        "    signal_names = read_signal_names(hea_path)\n",
        "    if signal_names is None:\n",
        "        print(f\"Warning: Failed to read signal names for record {record_id}. Skipping.\")\n",
        "        return []\n",
        "\n",
        "    signal_data = load_signal_data(mat_path)\n",
        "    if signal_data is None:\n",
        "        print(f\"Warning: Failed to load signal data for record {record_id}. Skipping.\")\n",
        "        return []\n",
        "    if signal_data.shape[0] != len(signal_names):\n",
        "        print(f\"Warning: Signal data channels ({signal_data.shape[0]}) mismatch HEA names ({len(signal_names)}) for {record_id}. Skipping.\")\n",
        "        return []\n",
        "    total_samples = signal_data.shape[1]\n",
        "\n",
        "    try:\n",
        "        event_patterns_re = [re.compile(p) for p in config_dict['apnea_hypopnea_patterns']]\n",
        "    except Exception as e:\n",
        "        print(f\"Error compiling regex patterns for {record_id}: {e}. Skipping.\")\n",
        "        return []\n",
        "\n",
        "    apnea_hypopnea_events = load_apnea_hypopnea_events_wfdb(\n",
        "        record_base_path,\n",
        "        event_patterns_re,\n",
        "        annotation_extension=config_dict.get('arousal_suffix','').replace('-','').replace('.mat','')\n",
        "    )\n",
        "    if apnea_hypopnea_events is None:\n",
        "        # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Warning: Could not load relevant events. Treating as no events.\") # Optional log\n",
        "        apnea_hypopnea_events = []\n",
        "    # else:\n",
        "        # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Loaded {len(apnea_hypopnea_events)} apnea/hypopnea events.\") # Optional log\n",
        "\n",
        "    signal_name_to_index = {name: i for i, name in enumerate(signal_names)}\n",
        "    try:\n",
        "        spec_indices = [signal_name_to_index[name] for name in config_dict['spectrogram_signals']]\n",
        "        raw_ts_indices = [signal_name_to_index[name] for name in config_dict['raw_ts_signals']]\n",
        "    except KeyError as e:\n",
        "        print(f\"Warning: Required signal '{e}' not found in record {record_id}. Skipping record.\")\n",
        "        return []\n",
        "\n",
        "    window_count = 0\n",
        "    sr = config_dict.get('sampling_rate', SAMPLING_RATE)\n",
        "    window_samples = config_dict['window_samples']\n",
        "    step_samples = config_dict['window_step_samples']\n",
        "    # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Windowing parameters: SR={sr}, WinSamples={window_samples}, StepSamples={step_samples}\") # Optional log\n",
        "\n",
        "    for start_sample in range(0, total_samples - window_samples + 1, step_samples):\n",
        "        end_sample = start_sample + window_samples\n",
        "        window_id = f\"{record_id}_w{window_count:06d}\"\n",
        "        # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Processing window {window_id} ({start_sample}-{end_sample})\") # Debug log (optional)\n",
        "\n",
        "        window_label = 0\n",
        "        for event_start, event_end in apnea_hypopnea_events:\n",
        "            if max(start_sample, event_start) < min(end_sample, event_end):\n",
        "                window_label = 1; break\n",
        "\n",
        "        modalities = {'window_id': window_id, 'record_id': record_id, 'label': window_label, 'split': split_name_for_saving}\n",
        "        modalities.update(demo_meta_record if demo_meta_record else {})\n",
        "        modalities_saved_ok = True; spec_paths = {}; raw_ts_paths = {}\n",
        "\n",
        "        for i, signal_idx in enumerate(spec_indices):\n",
        "            signal_name = config_dict['spectrogram_signals'][i]\n",
        "            signal_window_data = signal_data[signal_idx, start_sample:end_sample]\n",
        "            spec_filename = f\"{window_id}_{signal_name}_spec.npy\"\n",
        "            spec_rel_path = os.path.join(\"spectrograms\", signal_name, spec_filename)\n",
        "            spec_full_path = os.path.join(base_save_path, split_name_for_saving, spec_rel_path)\n",
        "\n",
        "            # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Generating spectrogram for {signal_name}, window {window_id}\") # Debug log (optional)\n",
        "            success = generate_and_save_numerical_spectrogram(\n",
        "                signal_window_data, sr, spec_full_path,\n",
        "                config_dict.get('spec_nperseg', SPECTROGRAM_NPERSEG),\n",
        "                config_dict.get('spec_noverlap', SPECTROGRAM_NOVERLAP),\n",
        "                config_dict['spectrogram_target_size'])\n",
        "            if success:\n",
        "                spec_paths[f'spec_{signal_name}_path'] = spec_rel_path\n",
        "            else:\n",
        "                print(f\"Warning: Failed to save spectrogram for {signal_name}, window {window_id} of record {record_id}. Skipping window.\")\n",
        "                modalities_saved_ok = False; break\n",
        "        if not modalities_saved_ok: continue\n",
        "\n",
        "        modalities.update(spec_paths)\n",
        "\n",
        "        for i, signal_idx in enumerate(raw_ts_indices):\n",
        "            signal_name = config_dict['raw_ts_signals'][i]\n",
        "            raw_segment = signal_data[signal_idx, start_sample:end_sample]\n",
        "            raw_filename = f\"{window_id}_{signal_name}_rawts.npy\"\n",
        "            raw_rel_path = os.path.join(\"raw_ts\", raw_filename)\n",
        "            raw_full_path = os.path.join(base_save_path, split_name_for_saving, raw_rel_path)\n",
        "\n",
        "            # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Preparing to save raw TS for {signal_name}, window {window_id} to {raw_full_path}\") # Debug log (optional)\n",
        "            try:\n",
        "                if len(raw_segment) == window_samples:\n",
        "                    os.makedirs(os.path.dirname(raw_full_path), exist_ok=True)\n",
        "                    np.save(raw_full_path, raw_segment.astype(np.float32))\n",
        "                    # print(f\"PROCESS_RECORD_WINDOWED: {record_id} - Successfully saved raw TS for {signal_name}, window {window_id}\") # Debug log (optional)\n",
        "                    raw_ts_paths[f'raw_{signal_name}_path'] = raw_rel_path\n",
        "                else:\n",
        "                    print(f\"Warning: Raw segment length mismatch for {signal_name}, window {window_id} of record {record_id}. Expected {window_samples}, got {len(raw_segment)}. Skipping window.\")\n",
        "                    modalities_saved_ok = False; break\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving raw TS {raw_filename} for record {record_id}: {e}. Skipping window.\")\n",
        "                modalities_saved_ok = False; break\n",
        "        if not modalities_saved_ok: continue\n",
        "\n",
        "        modalities.update(raw_ts_paths)\n",
        "        processed_windows_metadata.append(modalities)\n",
        "        window_count += 1\n",
        "\n",
        "    del signal_data; gc.collect()\n",
        "    # print(f\"PROCESS_RECORD_WINDOWED: Finished processing record_id: {record_id}. Generated {window_count} windows. PID: {os.getpid()}\") # Debug log (optional)\n",
        "    return processed_windows_metadata\n",
        "\n",
        "\n",
        "# --- Data Loading for the Windowed Training ---\n",
        "def data_generator_windowed(metadata_df, config_dict, batch_size_val, scaler=None, feature_cols_list=None, is_training=True, data_base_path_val=None):\n",
        "    num_samples_original = len(metadata_df)\n",
        "    data_base_path = data_base_path_val if data_base_path_val else config_dict.get('training_data_dir', config_dict['persistent_output_dir_base'])\n",
        "\n",
        "    tab_feature_cols = feature_cols_list if feature_cols_list else [col for col in config_dict['base_tab_features'] if col in metadata_df.columns]\n",
        "    num_tab_features = len(tab_feature_cols)\n",
        "\n",
        "    if is_training and config_dict.get('ENABLE_OVERSAMPLING', False) and 'label' in metadata_df.columns:\n",
        "        positive_df = metadata_df[metadata_df['label'] == 1]; negative_df = metadata_df[metadata_df['label'] == 0]\n",
        "        positive_indices = positive_df.index.tolist(); negative_indices = negative_df.index.tolist()\n",
        "        if positive_indices and negative_indices:\n",
        "            target_positive_ratio = config_dict.get('OVERSAMPLING_TARGET_POSITIVE_RATIO', 0.3)\n",
        "            num_epoch_samples = num_samples_original\n",
        "            num_pos_epoch = int(num_epoch_samples * target_positive_ratio)\n",
        "            num_neg_epoch = num_epoch_samples - num_pos_epoch\n",
        "\n",
        "            oversampled_pos_indices = np.random.choice(positive_indices, size=num_pos_epoch, replace=True) if num_pos_epoch > 0 else np.array([], dtype=int)\n",
        "            oversampled_neg_indices = np.random.choice(negative_indices, size=num_neg_epoch, replace=num_neg_epoch > len(negative_indices)) if num_neg_epoch > 0 else np.array([], dtype=int)\n",
        "\n",
        "            epoch_indices = np.concatenate([oversampled_pos_indices, oversampled_neg_indices]); np.random.shuffle(epoch_indices)\n",
        "            metadata_to_iterate_from = metadata_df; num_samples_for_iteration = len(epoch_indices)\n",
        "        else:\n",
        "            epoch_indices = np.arange(num_samples_original); np.random.shuffle(epoch_indices)\n",
        "            metadata_to_iterate_from = metadata_df; num_samples_for_iteration = num_samples_original\n",
        "    else:\n",
        "        epoch_indices = np.arange(num_samples_original)\n",
        "        if is_training: np.random.shuffle(epoch_indices)\n",
        "        metadata_to_iterate_from = metadata_df; num_samples_for_iteration = num_samples_original\n",
        "\n",
        "    for start_idx in range(0, num_samples_for_iteration, batch_size_val):\n",
        "        current_batch_original_indices = epoch_indices[start_idx : min(start_idx + batch_size_val, num_samples_for_iteration)]\n",
        "        actual_batch_size = len(current_batch_original_indices)\n",
        "        if actual_batch_size == 0: continue\n",
        "\n",
        "        batch_specs_np = {f'spec_{name}': np.zeros((actual_batch_size,) + config_dict['spectrogram_target_size'] + (3,), dtype=np.float32) for name in config_dict['spectrogram_signals']}\n",
        "        batch_raw_ts_np = {f'raw_{name}': np.zeros((actual_batch_size, config_dict['window_samples']), dtype=np.float32) for name in config_dict['raw_ts_signals']}\n",
        "        if num_tab_features > 0:\n",
        "            batch_tabular_np = np.zeros((actual_batch_size, num_tab_features), dtype=np.float32)\n",
        "        # Else, batch_tabular_np is not created if the num_tab_features is 0, it won't be yielded.\n",
        "\n",
        "        batch_labels_np = np.zeros((actual_batch_size, 1), dtype=np.int32)\n",
        "        valid_windows_in_batch_count = 0\n",
        "\n",
        "        for i_in_batch, original_df_idx in enumerate(current_batch_original_indices):\n",
        "            try:\n",
        "                row = metadata_to_iterate_from.iloc[original_df_idx]\n",
        "            except IndexError:\n",
        "                continue\n",
        "\n",
        "            valid_modalities = True; loaded_specs = {}; loaded_raw = {}\n",
        "            current_split_for_path = row.get('split', 'all_data_temp')\n",
        "            base_path_for_loading = os.path.join(data_base_path, current_split_for_path)\n",
        "            apply_augmentation = is_training\n",
        "\n",
        "            for signal_name in config_dict['spectrogram_signals']:\n",
        "                spec_path_key = f'spec_{signal_name}_path'\n",
        "                if spec_path_key not in row or pd.isna(row[spec_path_key]):\n",
        "                    valid_modalities=False; break\n",
        "                spec_path_npy = os.path.join(base_path_for_loading, row[spec_path_key])\n",
        "                spec_array = load_numerical_spectrogram(spec_path_npy, config_dict['spectrogram_target_size'], augment=apply_augmentation, config_params=config_dict)\n",
        "                if np.sum(np.isnan(spec_array)) > 0 : # Check for NaNs in NumPy array\n",
        "                    valid_modalities=False; break\n",
        "                loaded_specs[signal_name] = spec_array\n",
        "            if not valid_modalities: continue\n",
        "\n",
        "            for signal_name in config_dict['raw_ts_signals']:\n",
        "                raw_path_key = f'raw_{signal_name}_path'\n",
        "                if raw_path_key not in row or pd.isna(row[raw_path_key]):\n",
        "                    valid_modalities=False; break\n",
        "                raw_path_npy = os.path.join(base_path_for_loading, row[raw_path_key])\n",
        "                raw_segment = load_npy(raw_path_npy, expected_shape=(config_dict['window_samples'],), augment=apply_augmentation, config_params=config_dict)\n",
        "                if raw_segment is None:\n",
        "                    valid_modalities=False; break\n",
        "                loaded_raw[signal_name] = raw_segment\n",
        "            if not valid_modalities: continue\n",
        "\n",
        "            current_tab_data = None # Initialize for this window\n",
        "            if num_tab_features > 0:\n",
        "                try:\n",
        "                    current_tab_data = row[tab_feature_cols].values.astype(np.float32)\n",
        "                    if np.isnan(current_tab_data).any():\n",
        "                        current_tab_data = np.nan_to_num(current_tab_data, nan=0.0)\n",
        "                except Exception:\n",
        "                    valid_modalities = False # Skip window if tabular data fails for it\n",
        "            if not valid_modalities: continue\n",
        "\n",
        "\n",
        "            try:\n",
        "                label = int(row['label'])\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            current_idx_in_batch = valid_windows_in_batch_count\n",
        "            for s_name in config_dict['spectrogram_signals']: batch_specs_np[f'spec_{s_name}'][current_idx_in_batch] = loaded_specs[s_name]\n",
        "            for s_name in config_dict['raw_ts_signals']: batch_raw_ts_np[f'raw_{s_name}'][current_idx_in_batch] = loaded_raw[s_name]\n",
        "            if num_tab_features > 0 and current_tab_data is not None: # Ensure current_tab_data was successfully processed\n",
        "                batch_tabular_np[current_idx_in_batch] = current_tab_data\n",
        "            elif num_tab_features > 0 and current_tab_data is None: # Should not happen if logic above is correct\n",
        "                print(f\"Warning: Tabular data expected but not loaded for window {row.get('window_id', 'Unknown')}. Filling with zeros.\")\n",
        "                batch_tabular_np[current_idx_in_batch] = np.zeros(num_tab_features, dtype=np.float32)\n",
        "\n",
        "\n",
        "            batch_labels_np[current_idx_in_batch, 0] = label\n",
        "            valid_windows_in_batch_count += 1\n",
        "\n",
        "        if valid_windows_in_batch_count == 0:\n",
        "            continue\n",
        "\n",
        "        if valid_windows_in_batch_count < actual_batch_size:\n",
        "            for s_name in config_dict['spectrogram_signals']: batch_specs_np[f'spec_{s_name}'] = batch_specs_np[f'spec_{s_name}'][:valid_windows_in_batch_count]\n",
        "            for s_name in config_dict['raw_ts_signals']: batch_raw_ts_np[f'raw_{s_name}'] = batch_raw_ts_np[f'raw_{s_name}'][:valid_windows_in_batch_count]\n",
        "            if num_tab_features > 0: # Only trim if batch_tabular_np was created\n",
        "                batch_tabular_np = batch_tabular_np[:valid_windows_in_batch_count]\n",
        "            batch_labels_np = batch_labels_np[:valid_windows_in_batch_count]\n",
        "\n",
        "        # Prepare model inputs dictionary\n",
        "        model_inputs = {}\n",
        "        for s_name in config_dict['spectrogram_signals']: model_inputs[f\"spec_{s_name}_input\"] = batch_specs_np[f'spec_{s_name}']\n",
        "        for s_name in config_dict['raw_ts_signals']: model_inputs[f\"raw_{s_name}_input\"] = np.expand_dims(batch_raw_ts_np[f'raw_{s_name}'], axis=-1)\n",
        "\n",
        "        if num_tab_features > 0:\n",
        "            # Apply scaling to the tabular data if scaler is provided and features exist\n",
        "            if scaler and hasattr(scaler, 'mean_') and batch_tabular_np.shape[0] > 0 and batch_tabular_np.shape[1] == scaler.n_features_in_:\n",
        "                try:\n",
        "                    batch_tabular_np_scaled = scaler.transform(batch_tabular_np)\n",
        "                    model_inputs[\"tabular_input\"] = batch_tabular_np_scaled\n",
        "                except Exception:\n",
        "                    model_inputs[\"tabular_input\"] = batch_tabular_np # Use unscaled if error\n",
        "            else: # No scaler or mismatch\n",
        "                 model_inputs[\"tabular_input\"] = batch_tabular_np\n",
        "\n",
        "\n",
        "        yield model_inputs, batch_labels_np\n",
        "\n",
        "# --- Create TF Dataset Function ---\n",
        "def create_tf_dataset_windowed(metadata_df, config_dict, batch_size_val, scaler, feature_cols_list, is_training=True, repeat_for_fit=False, data_base_path_val=None):\n",
        "    final_feature_cols = [col for col in feature_cols_list if col in metadata_df.columns]\n",
        "    num_final_tab_features = len(final_feature_cols)\n",
        "\n",
        "    # Define the output signature for the tf.data.Dataset\n",
        "    output_signature_inputs = {} # Start empty\n",
        "    for name in config_dict['spectrogram_signals']:\n",
        "        output_signature_inputs[f\"spec_{name}_input\"] = tf.TensorSpec(shape=(None,) + config_dict['spectrogram_target_size'] + (3,), dtype=tf.float32)\n",
        "    for name in config_dict['raw_ts_signals']:\n",
        "        output_signature_inputs[f\"raw_{name}_input\"] = tf.TensorSpec(shape=(None, config_dict['window_samples'], 1), dtype=tf.float32)\n",
        "\n",
        "    if num_final_tab_features > 0: # Conditionally add tabular input signature\n",
        "        output_signature_inputs[\"tabular_input\"] = tf.TensorSpec(shape=(None, num_final_tab_features), dtype=tf.float32)\n",
        "\n",
        "    output_signature = (output_signature_inputs, tf.TensorSpec(shape=(None, 1), dtype=tf.int32))\n",
        "\n",
        "    required_paths = [f'spec_{name}_path' for name in config_dict['spectrogram_signals']] + \\\n",
        "                     [f'raw_{name}_path' for name in config_dict['raw_ts_signals']]\n",
        "    # Essential columns for filtering (tabular features are only essential if they are going to be used)\n",
        "    essential_cols_for_filtering = ['split', 'label'] + required_paths\n",
        "    if num_final_tab_features > 0:\n",
        "        essential_cols_for_filtering += final_feature_cols\n",
        "\n",
        "    metadata_df_filtered = metadata_df.copy()\n",
        "    cols_to_check_for_na = [col for col in essential_cols_for_filtering if col in metadata_df_filtered.columns and col != 'split']\n",
        "    metadata_df_filtered.dropna(subset=cols_to_check_for_na, inplace=True)\n",
        "    metadata_df_filtered.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if len(metadata_df_filtered) < len(metadata_df):\n",
        "        print(f\"Filtered {len(metadata_df) - len(metadata_df_filtered)} rows from metadata due to missing essential data for tf.data.Dataset.\")\n",
        "    if len(metadata_df_filtered) == 0:\n",
        "        print(\"Error: Metadata is empty after filtering for essential columns. Cannot create tf.data.Dataset.\")\n",
        "        return None\n",
        "\n",
        "    gen_lambda = lambda: data_generator_windowed(\n",
        "        metadata_df_filtered,\n",
        "        config_dict,\n",
        "        batch_size_val,\n",
        "        scaler,\n",
        "        final_feature_cols, # Pass the determined final_feature_cols\n",
        "        is_training=is_training,\n",
        "        data_base_path_val=data_base_path_val\n",
        "    )\n",
        "    dataset = tf.data.Dataset.from_generator(gen_lambda, output_signature=output_signature)\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=max(1000, batch_size_val*10)).repeat()\n",
        "    elif repeat_for_fit:\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# --- Windowed Model Definition ---\n",
        "def build_multimodal_model_windowed(config_dict, num_tabular_features): # num_tabular_features is len(final_feature_cols)\n",
        "    l2_reg = config_dict.get('l2_reg_factor', 1e-5)\n",
        "\n",
        "    spectrogram_inputs = [keras.Input(shape=config_dict['spectrogram_target_size'] + (3,), name=f\"spec_{name}_input\") for name in config_dict['spectrogram_signals']]\n",
        "    raw_ts_inputs = [keras.Input(shape=(config_dict['window_samples'], 1), name=f\"raw_{name}_input\") for name in config_dict['raw_ts_signals']]\n",
        "\n",
        "    model_inputs_list = spectrogram_inputs + raw_ts_inputs\n",
        "\n",
        "    tabular_input_layer = None # Keep track of the Keras Input layer for the tabular data\n",
        "    if num_tabular_features > 0:\n",
        "        tabular_input_layer = keras.Input(shape=(num_tabular_features,), name=\"tabular_input\")\n",
        "        model_inputs_list.append(tabular_input_layer)\n",
        "\n",
        "    spectrogram_cnn_outputs = []\n",
        "    for i, spec_input_raw in enumerate(spectrogram_inputs):\n",
        "        signal_name = config_dict['spectrogram_signals'][i]\n",
        "        spec_input_processed = tf.keras.applications.mobilenet_v2.preprocess_input(spec_input_raw)\n",
        "        base_model = keras.applications.MobileNetV2(\n",
        "            input_shape=config_dict['spectrogram_target_size']+(3,),\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            pooling='avg',\n",
        "            name=f\"mobilenet_{signal_name}\"\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        img_features = base_model(spec_input_processed, training=False)\n",
        "        img_branch = layers.Dropout(0.5, name=f\"dropout_spec_{signal_name}\")(img_features)\n",
        "        img_branch = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg), name=f\"spec_features_{signal_name}\")(img_branch)\n",
        "        spectrogram_cnn_outputs.append(img_branch)\n",
        "\n",
        "    raw_ts_cnn_outputs = []\n",
        "    for i, raw_input_val in enumerate(raw_ts_inputs):\n",
        "        signal_name = config_dict['raw_ts_signals'][i]\n",
        "        ts_branch = layers.Conv1D(32, 16, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg), name=f\"ts_conv1_{signal_name}\")(raw_input_val)\n",
        "        ts_branch = layers.BatchNormalization(name=f\"ts_bn1_{signal_name}\")(ts_branch)\n",
        "        ts_branch = layers.MaxPooling1D(4, name=f\"ts_pool1_{signal_name}\")(ts_branch)\n",
        "        ts_branch = layers.Conv1D(64, 16, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg), name=f\"ts_conv2_{signal_name}\")(ts_branch)\n",
        "        ts_branch = layers.BatchNormalization(name=f\"ts_bn2_{signal_name}\")(ts_branch)\n",
        "        ts_branch = layers.GlobalMaxPooling1D(name=f\"ts_features_{signal_name}\")(ts_branch)\n",
        "        ts_branch = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg), name=f\"ts_dense_{signal_name}\")(ts_branch)\n",
        "        raw_ts_cnn_outputs.append(ts_branch)\n",
        "\n",
        "    features_to_fuse = spectrogram_cnn_outputs + raw_ts_cnn_outputs\n",
        "\n",
        "    if num_tabular_features > 0 and tabular_input_layer is not None: # Check if the Input layer was created\n",
        "        tab_branch = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(tabular_input_layer) # Use the Input layer here\n",
        "        tab_branch = layers.BatchNormalization()(tab_branch)\n",
        "        tab_branch = layers.Dropout(0.3)(tab_branch)\n",
        "        tab_branch = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg), name=\"tabular_features\")(tab_branch)\n",
        "        features_to_fuse.append(tab_branch)\n",
        "\n",
        "    if not features_to_fuse:\n",
        "        raise ValueError(\"No input branches to fuse! Check signal and tabular feature configurations.\")\n",
        "\n",
        "    fused_features = features_to_fuse[0] if len(features_to_fuse) == 1 else layers.Concatenate(name=\"fused_features\")(features_to_fuse)\n",
        "\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(fused_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    output = layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=model_inputs_list, outputs=output, name=\"windowed_multimodal_classifier\")\n",
        "    return model\n",
        "\n",
        "# Function to Engineer the Patient-Level Features ---\n",
        "def engineer_patient_features(window_df, tuned_window_threshold, proba_col='win_proba'):\n",
        "    \"\"\"\n",
        "    Engineers patient-level features from window-level predictions.\n",
        "    Args:\n",
        "        window_df (pd.DataFrame): DataFrame with window predictions, must include 'record_id' and proba_col.\n",
        "        tuned_window_threshold (float): The threshold to consider a window as an \"event\".\n",
        "        proba_col (str): Name of the column containing window probabilities.\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with one row per patient and engineered features.\n",
        "    \"\"\"\n",
        "    if window_df.empty or proba_col not in window_df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Create the binary predictions based on the tuned threshold\n",
        "    window_df['win_pred_binary_event'] = (window_df[proba_col] > tuned_window_threshold).astype(int)\n",
        "\n",
        "    patient_features_list = []\n",
        "    for record_id, group in window_df.groupby('record_id'):\n",
        "        features = {'record_id': record_id}\n",
        "\n",
        "        # Feature 1: Positive Window Ratio (fraction of windows predicted as event)\n",
        "        features['positive_window_ratio'] = group['win_pred_binary_event'].mean()\n",
        "\n",
        "        # Feature 2: Mean probability of windows predicted as positive\n",
        "        positive_windows = group[group['win_pred_binary_event'] == 1]\n",
        "        if not positive_windows.empty:\n",
        "            features['mean_event_proba'] = positive_windows[proba_col].mean()\n",
        "        else:\n",
        "            features['mean_event_proba'] = 0.0 # Or np.nan, or a low value\n",
        "\n",
        "        # Feature 3: Standard deviation of all window probabilities for the patient\n",
        "        features['std_proba'] = group[proba_col].std()\n",
        "\n",
        "        # Feature 4: Median of all window probabilities for the patient\n",
        "        features['median_proba'] = group[proba_col].median()\n",
        "\n",
        "        # Feature 5: 90th percentile of window probabilities\n",
        "        features['p90_proba'] = group[proba_col].quantile(0.90)\n",
        "\n",
        "        # Feature 6: Number of \"event segments\" (consecutive windows predicted as event)\n",
        "        # This is a bit more complex; for simplicity, we'll use a basic version\n",
        "        # A segment is a run of 1s in 'win_pred_binary_event'\n",
        "        group['is_event_shifted'] = group['win_pred_binary_event'].shift(1, fill_value=0)\n",
        "        group['segment_start'] = (group['win_pred_binary_event'] == 1) & (group['is_event_shifted'] == 0)\n",
        "        features['num_event_segments'] = group['segment_start'].sum()\n",
        "\n",
        "        patient_features_list.append(features)\n",
        "\n",
        "    patient_features_df = pd.DataFrame(patient_features_list)\n",
        "    # Handle NaNs that might arise from std (if only 1 window) or if no positive windows for mean_event_proba\n",
        "    patient_features_df.fillna(0.0, inplace=True)\n",
        "    return patient_features_df\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    main_start_time = time.time()\n",
        "    config = {\n",
        "        'data_dir': DATA_DIR,\n",
        "        'persistent_output_dir_base': PERSISTENT_OUTPUT_DIR_BASE,\n",
        "        'training_data_dir': LOCAL_DATA_DIR_BASE,\n",
        "        'final_model_output_dir': FINAL_MODEL_RESULTS_DIR,\n",
        "        'labels_file': LABELS_FILE, 'demo_file': DEMOGRAPHICS_FILE,\n",
        "        'arousal_suffix': AROUSAL_MAT_SUFFIX.replace('-','').replace('.mat',''),\n",
        "        'spectrogram_signals': SPECTROGRAM_SIGNALS, 'raw_ts_signals': RAW_TS_SIGNALS,\n",
        "        'base_tab_features': BASE_TABULAR_FEATURES,\n",
        "        'spectrogram_target_size': SPECTROGRAM_TARGET_SIZE,\n",
        "        'sampling_rate': SAMPLING_RATE,\n",
        "        'window_seconds': WINDOW_SECONDS, 'window_samples': WINDOW_SAMPLES,\n",
        "        'window_overlap_ratio': WINDOW_OVERLAP_RATIO, 'window_step_samples': WINDOW_STEP_SAMPLES,\n",
        "        'spec_nperseg': SPECTROGRAM_NPERSEG, 'spec_noverlap': SPECTROGRAM_NOVERLAP,\n",
        "        'learning_rate': DEFAULT_LEARNING_RATE,\n",
        "        'fine_tune_lr_factor': DEFAULT_FINE_TUNE_LR_FACTOR,\n",
        "        'fine_tune_layers': DEFAULT_FINE_TUNE_LAYERS,\n",
        "        'focal_loss_gamma': DEFAULT_FOCAL_LOSS_GAMMA,\n",
        "        'l2_reg_factor': DEFAULT_L2_REG_FACTOR,\n",
        "        'ENABLE_OVERSAMPLING': DEFAULT_ENABLE_OVERSAMPLING,\n",
        "        'OVERSAMPLING_TARGET_POSITIVE_RATIO': DEFAULT_OVERSAMPLING_TARGET_POSITIVE_RATIO,\n",
        "        'AUGMENT_SPECTROGRAMS': DEFAULT_AUGMENT_SPECTROGRAMS,\n",
        "        'AUGMENT_RAW_TS': DEFAULT_AUGMENT_RAW_TS,\n",
        "        'INITIAL_EPOCHS': GLOBAL_INITIAL_EPOCHS,\n",
        "        'FINE_TUNE_EPOCHS': GLOBAL_FINE_TUNE_EPOCHS,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "        'apnea_hypopnea_patterns': [\n",
        "            r\"\\(?resp_centralapnea\\)?\", r\"\\(?resp_hypopnea\\)?\",\n",
        "            r\"\\(?resp_obstructiveapnea\\)?\", r\"\\(?resp_mixedapnea\\)?\", r\"mixed apnea\"\n",
        "        ],\n",
        "        'binary_target_col': PATIENT_BINARY_TARGET_COL,\n",
        "        'patient_multiclass_target_col': PATIENT_SEVERITY_TARGET_COL,\n",
        "        'patient_ahi_threshold_binary': 5,\n",
        "        'patient_ahi_thresholds_multiclass': [5, 15, 30],\n",
        "        'PATIENT_SEVERITY_THRESHOLDS_ON_RATIO': list(PATIENT_SEVERITY_THRESHOLDS_ON_RATIO),\n",
        "        'USE_SECONDARY_PATIENT_MODEL': USE_SECONDARY_PATIENT_MODEL\n",
        "    }\n",
        "    prepare_output_dirs_windowed(config['persistent_output_dir_base'], config['training_data_dir'])\n",
        "\n",
        "    print(\"--- STAGE 1: Data Preprocessing and Windowing ---\")\n",
        "    patient_true_labels_df, demo_dict = load_labels_and_metadata(\n",
        "        config['labels_file'], config['demo_file'],\n",
        "        patient_binary_target_col=config['binary_target_col'],\n",
        "        patient_multiclass_target_col=config['patient_multiclass_target_col'],\n",
        "        ahi_threshold_binary=config['patient_ahi_threshold_binary'],\n",
        "        ahi_thresholds_multiclass=config['patient_ahi_thresholds_multiclass']\n",
        "    )\n",
        "\n",
        "    all_records_in_dir = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    valid_record_ids_stage1 = set()\n",
        "    if not patient_true_labels_df.empty and 'Record' in patient_true_labels_df.columns:\n",
        "        valid_record_ids_stage1.update(patient_true_labels_df['Record'].astype(str).unique())\n",
        "    if demo_dict:\n",
        "        valid_record_ids_stage1.update(demo_dict.keys())\n",
        "\n",
        "    all_valid_records_for_processing = [r for r in all_records_in_dir if r in valid_record_ids_stage1]\n",
        "    print(f\"Found {len(all_valid_records_for_processing)} records in '{DATA_DIR}' that have corresponding entries in labels/demographics files.\")\n",
        "\n",
        "    if not all_valid_records_for_processing:\n",
        "        print(f\"Error: No valid records found for Stage 1 processing. Check paths and content. Exiting.\"); exit()\n",
        "\n",
        "    subset_records_for_stage1 = all_valid_records_for_processing\n",
        "    if MAX_RECORDS_TO_PROCESS > 0 and MAX_RECORDS_TO_PROCESS < len(all_valid_records_for_processing):\n",
        "        print(f\"Selecting {MAX_RECORDS_TO_PROCESS} records RANDOMLY for processing.\")\n",
        "        random.seed(RANDOM_STATE);\n",
        "        subset_records_for_stage1 = random.sample(all_valid_records_for_processing, MAX_RECORDS_TO_PROCESS)\n",
        "\n",
        "    subset_records_array = np.array(subset_records_for_stage1)\n",
        "\n",
        "    all_windows_meta_path = os.path.join(config['persistent_output_dir_base'], PERSISTENT_METADATA_CSV_NAME)\n",
        "    all_windows_df = pd.DataFrame()\n",
        "    local_all_data_temp_path = os.path.join(config['training_data_dir'], \"all_data_temp\")\n",
        "\n",
        "    if not FORCE_REPROCESS_ALL_WINDOWS and os.path.exists(all_windows_meta_path):\n",
        "        print(f\"Attempting to load existing metadata from: {all_windows_meta_path}\")\n",
        "        all_windows_df = pd.read_csv(all_windows_meta_path)\n",
        "        if 'record_id' in all_windows_df.columns:\n",
        "            all_windows_df['record_id'] = all_windows_df['record_id'].astype(str)\n",
        "\n",
        "        if not set(subset_records_array).issubset(set(all_windows_df['record_id'].unique())):\n",
        "            print(\"Warning: Existing metadata does not cover all currently selected records. Reprocessing NPY files.\")\n",
        "            FORCE_REPROCESS_ALL_WINDOWS = True\n",
        "        else:\n",
        "            raw_ts_path_check = os.path.join(local_all_data_temp_path, \"raw_ts\")\n",
        "            if not os.path.exists(raw_ts_path_check) or not os.listdir(raw_ts_path_check):\n",
        "                 print(f\"Warning: Metadata found, but local NPY data at '{local_all_data_temp_path}' (specifically 'raw_ts' subdir) seems missing/empty. Reprocessing NPY files.\")\n",
        "                 FORCE_REPROCESS_ALL_WINDOWS = True\n",
        "            else:\n",
        "                 print(\"Metadata loaded. Assuming local NPY files are present. Skipping NPY generation.\")\n",
        "    else:\n",
        "        if FORCE_REPROCESS_ALL_WINDOWS: print(\"FORCE_REPROCESS_ALL_WINDOWS is True. Reprocessing NPY files.\")\n",
        "        else: print(\"Existing processed metadata not found or incomplete. Reprocessing NPY files.\")\n",
        "        FORCE_REPROCESS_ALL_WINDOWS = True\n",
        "\n",
        "\n",
        "    if FORCE_REPROCESS_ALL_WINDOWS:\n",
        "        print(f\"Processing {len(subset_records_array)} records into windows (NPY files)...\")\n",
        "        if os.path.exists(local_all_data_temp_path):\n",
        "            shutil.rmtree(local_all_data_temp_path)\n",
        "        os.makedirs(os.path.join(local_all_data_temp_path, \"raw_ts\"), exist_ok=True)\n",
        "        for signal_name in SPECTROGRAM_SIGNALS:\n",
        "            os.makedirs(os.path.join(local_all_data_temp_path, \"spectrograms\", signal_name), exist_ok=True)\n",
        "\n",
        "        processing_start_time = time.time()\n",
        "        process_args_list = []\n",
        "        for record_id_proc in subset_records_array:\n",
        "            record_id_str = str(record_id_proc)\n",
        "            demo_meta = {}\n",
        "            if demo_dict and record_id_str in demo_dict:\n",
        "                demo_meta = {feat: demo_dict[record_id_str].get(feat, np.nan) for feat in config['base_tab_features']}\n",
        "            else:\n",
        "                demo_meta = {feat: np.nan for feat in config['base_tab_features']}\n",
        "\n",
        "            process_args_list.append((record_id_str, \"all_data_temp\", config, demo_meta, config['training_data_dir']))\n",
        "\n",
        "        num_processes = min(os.cpu_count(), 4) if os.cpu_count() else 2\n",
        "        print(f\"Using {num_processes} processes for window generation.\")\n",
        "\n",
        "        all_processed_windows_list_of_lists = []\n",
        "        if num_processes > 1 and __name__ == '__main__':\n",
        "            try:\n",
        "                with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "                    with tqdm(total=len(process_args_list), desc=\"Stage 1: Processing records (MP)\") as pbar:\n",
        "                        for result in pool.imap_unordered(process_record_windowed_mp_wrapper, process_args_list):\n",
        "                            if result:\n",
        "                                all_processed_windows_list_of_lists.append(result)\n",
        "                            pbar.update(1)\n",
        "            except Exception as mp_e:\n",
        "                print(f\"Multiprocessing failed with error: {mp_e}. Falling back to sequential processing.\")\n",
        "                all_processed_windows_list_of_lists = []\n",
        "                for args_item in tqdm(process_args_list, desc=\"Stage 1: Processing records (Sequential Fallback)\"):\n",
        "                    result = process_record_windowed_mp_wrapper(args_item)\n",
        "                    if result:\n",
        "                        all_processed_windows_list_of_lists.append(result)\n",
        "        else:\n",
        "            print(\"Executing record processing sequentially.\")\n",
        "            for args_item in tqdm(process_args_list, desc=\"Stage 1: Processing records (Sequential)\"):\n",
        "                result = process_record_windowed_mp_wrapper(args_item)\n",
        "                if result:\n",
        "                    all_processed_windows_list_of_lists.append(result)\n",
        "\n",
        "        all_processed_windows_list = [item for sublist in all_processed_windows_list_of_lists if sublist for item in sublist]\n",
        "        print(f\"\\nWindow processing (Stage 1) took: {time.time() - processing_start_time:.2f} seconds.\")\n",
        "        print(f\"Total windows generated: {len(all_processed_windows_list)}\")\n",
        "\n",
        "        if not all_processed_windows_list:\n",
        "            print(\"Error: No windows were processed. Check logs. Exiting.\"); exit()\n",
        "\n",
        "        all_windows_df = pd.DataFrame(all_processed_windows_list)\n",
        "        if not all_windows_df.empty:\n",
        "            all_windows_df['record_id'] = all_windows_df['record_id'].astype(str)\n",
        "            all_windows_df.to_csv(all_windows_meta_path, index=False)\n",
        "            print(f\"Saved all processed windows metadata to: {all_windows_meta_path}\")\n",
        "\n",
        "        del all_processed_windows_list, all_processed_windows_list_of_lists; gc.collect()\n",
        "\n",
        "    if all_windows_df.empty:\n",
        "        print(\"Error: Window data DataFrame is empty after Stage 1. Cannot proceed. Exiting.\"); exit()\n",
        "    print(\"--- Stage 1 Finished ---\")\n",
        "\n",
        "    print(\"\\n--- FINAL MODEL TRAINING AND EVALUATION STAGE ---\")\n",
        "\n",
        "    record_to_label_map_binary = {}\n",
        "    record_to_label_map_multiclass = {}\n",
        "    if not patient_true_labels_df.empty and 'Record' in patient_true_labels_df.columns and config['binary_target_col'] in patient_true_labels_df.columns:\n",
        "        record_to_label_map_binary = {str(r):l for r,l in patient_true_labels_df.set_index('Record')[config['binary_target_col']].to_dict().items()}\n",
        "    if not patient_true_labels_df.empty and 'Record' in patient_true_labels_df.columns and config['patient_multiclass_target_col'] in patient_true_labels_df.columns:\n",
        "        record_to_label_map_multiclass = {str(r):l for r,l in patient_true_labels_df.set_index('Record')[config['patient_multiclass_target_col']].to_dict().items()}\n",
        "\n",
        "    subset_binary_labels_for_split = np.array([record_to_label_map_binary.get(str(rec_id), 0) for rec_id in subset_records_array])\n",
        "    stratify_final_split = subset_binary_labels_for_split if len(np.unique(subset_binary_labels_for_split)) > 1 else None\n",
        "\n",
        "    final_train_val_pool_ids, final_test_ids = train_test_split(\n",
        "        subset_records_array,\n",
        "        test_size=FINAL_HOLDOUT_TEST_SET_RATIO,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=stratify_final_split\n",
        "    )\n",
        "\n",
        "    final_train_val_multiclass_labels = np.array([record_to_label_map_multiclass.get(str(rec_id), 0) for rec_id in final_train_val_pool_ids])\n",
        "    stratify_val_split = final_train_val_multiclass_labels if len(np.unique(final_train_val_multiclass_labels)) > 1 else None\n",
        "\n",
        "    final_train_ids, final_val_ids = [], []\n",
        "    if len(final_train_val_pool_ids) > 0 :\n",
        "        if len(final_train_val_pool_ids) < 2 or FINAL_MODEL_VALIDATION_RATIO <= 0 or FINAL_MODEL_VALIDATION_RATIO >=1:\n",
        "            final_train_ids = final_train_val_pool_ids\n",
        "            print(\"Validation set not created. Using entire pool for training.\")\n",
        "        else:\n",
        "            final_train_ids, final_val_ids = train_test_split(\n",
        "                final_train_val_pool_ids,\n",
        "                test_size=FINAL_MODEL_VALIDATION_RATIO,\n",
        "                random_state=RANDOM_STATE,\n",
        "                stratify=stratify_val_split\n",
        "            )\n",
        "\n",
        "    print(f\"Final Data Split (by Record IDs): Train: {len(final_train_ids)}, Validation: {len(final_val_ids)}, Test: {len(final_test_ids)}\")\n",
        "\n",
        "    if len(final_train_ids) == 0:\n",
        "        print(\"Error: Final training set is empty. Exiting.\"); exit()\n",
        "\n",
        "    # Filter all_windows_df for train, val, test based on the patient IDs\n",
        "    # These DFs will be used for the window-level training and later for patient-level feature engineering\n",
        "    train_windows_df = all_windows_df[all_windows_df['record_id'].isin(final_train_ids)].copy(); train_windows_df.loc[:, 'split'] = 'all_data_temp'\n",
        "    val_windows_df = pd.DataFrame()\n",
        "    if len(final_val_ids) > 0:\n",
        "        val_windows_df = all_windows_df[all_windows_df['record_id'].isin(final_val_ids)].copy(); val_windows_df.loc[:, 'split'] = 'all_data_temp'\n",
        "    test_windows_df = pd.DataFrame()\n",
        "    if len(final_test_ids) > 0:\n",
        "        test_windows_df = all_windows_df[all_windows_df['record_id'].isin(final_test_ids)].copy(); test_windows_df.loc[:, 'split'] = 'all_data_temp'\n",
        "\n",
        "    print(f\"Window counts: Train: {len(train_windows_df)}, Validation: {len(val_windows_df)}, Test: {len(test_windows_df)}\")\n",
        "\n",
        "    final_feature_cols = [col for col in config['base_tab_features'] if col in train_windows_df.columns] # Use train_windows_df for column check\n",
        "    print(f\"Using tabular features for primary model: {final_feature_cols}\")\n",
        "    num_final_tab_features = len(final_feature_cols)\n",
        "\n",
        "    final_imputation_values = {}\n",
        "    if not train_windows_df.empty:\n",
        "        for col in final_feature_cols:\n",
        "            if train_windows_df[col].isnull().all():\n",
        "                final_imputation_values[col] = 0.0\n",
        "            else:\n",
        "                final_imputation_values[col] = train_windows_df[col].median()\n",
        "    print(f\"Imputation values for tabular features: {final_imputation_values}\")\n",
        "\n",
        "    for df_loop in [train_windows_df, val_windows_df, test_windows_df]:\n",
        "        if not df_loop.empty:\n",
        "            for col in final_feature_cols:\n",
        "                if col in df_loop.columns:\n",
        "                    df_loop.loc[:, col] = df_loop[col].fillna(final_imputation_values.get(col, 0.0))\n",
        "\n",
        "    final_scaler = None\n",
        "    if num_final_tab_features > 0 and not train_windows_df.empty:\n",
        "        numeric_train_tab = train_windows_df[final_feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
        "        if not numeric_train_tab.empty and numeric_train_tab.shape[1] > 0:\n",
        "             final_scaler = StandardScaler().fit(numeric_train_tab.values)\n",
        "             print(\"StandardScaler fitted on training tabular data.\")\n",
        "\n",
        "    class_weights = None\n",
        "    if not train_windows_df.empty and 'label' in train_windows_df.columns:\n",
        "        labels_train = train_windows_df['label'].values\n",
        "        unique_classes, class_counts = np.unique(labels_train, return_counts=True)\n",
        "        if len(unique_classes) == 2:\n",
        "            class_weights_arr = compute_class_weight(class_weight='balanced', classes=unique_classes, y=labels_train)\n",
        "            class_weights = dict(zip(map(int,unique_classes), class_weights_arr))\n",
        "        print(f\"Window-level class weights: {class_weights}\")\n",
        "\n",
        "    current_bs = config['BATCH_SIZE']\n",
        "    train_ds = create_tf_dataset_windowed(train_windows_df, config, current_bs, final_scaler, final_feature_cols, is_training=True, data_base_path_val=config['training_data_dir'])\n",
        "\n",
        "    val_ds_fit = None\n",
        "    if not val_windows_df.empty:\n",
        "        val_ds_fit = create_tf_dataset_windowed(val_windows_df, config, current_bs, final_scaler, final_feature_cols, is_training=False, repeat_for_fit=True, data_base_path_val=config['training_data_dir'])\n",
        "\n",
        "    if train_ds is None:\n",
        "        print(\"Error: Failed to create training tf.data.Dataset. Exiting.\"); exit()\n",
        "\n",
        "    train_steps = int(np.ceil(len(train_windows_df) / current_bs)) if not train_windows_df.empty and current_bs > 0 else 0\n",
        "    val_steps = int(np.ceil(len(val_windows_df) / current_bs)) if not val_windows_df.empty and current_bs > 0 else 0\n",
        "\n",
        "    if train_steps == 0:\n",
        "        print(\"Error: Training steps is 0. Exiting.\"); exit()\n",
        "\n",
        "    # Don't delete the train_windows_df, val_windows_df, test_windows_df yet, needed for patient-level feature engineering\n",
        "    # del train_windows_df; gc.collect()\n",
        "\n",
        "    final_model = build_multimodal_model_windowed(config, num_final_tab_features)\n",
        "    final_model.summary(line_length=120)\n",
        "\n",
        "    loss_fn = tf.keras.losses.BinaryFocalCrossentropy(gamma=config['focal_loss_gamma'])\n",
        "    metrics_list = [\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        "\n",
        "    final_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
        "        loss=loss_fn,\n",
        "        metrics=metrics_list\n",
        "    )\n",
        "\n",
        "    checkpoint_path = os.path.join(FINAL_MODEL_RESULTS_DIR, \"best_final_model.keras\")\n",
        "    cb_monitor = 'val_auc' if val_ds_fit and val_steps > 0 else 'auc'\n",
        "    cb_monitor_loss = 'val_loss' if val_ds_fit and val_steps > 0 else 'loss'\n",
        "\n",
        "    callbacks_p1 = [\n",
        "        keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor=cb_monitor, mode='max', verbose=1),\n",
        "        keras.callbacks.EarlyStopping(monitor=cb_monitor_loss, patience=5, verbose=1, mode='min', restore_best_weights=False),\n",
        "        keras.callbacks.ReduceLROnPlateau(monitor=cb_monitor_loss, factor=0.2, patience=3, verbose=1, min_lr=1e-6)\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n--- Phase 1 Training ({config['INITIAL_EPOCHS']} epochs) ---\")\n",
        "    print(f\"Monitoring '{cb_monitor}' for ModelCheckpoint and '{cb_monitor_loss}' for EarlyStopping/ReduceLROnPlateau.\")\n",
        "    hist_p1 = final_model.fit(\n",
        "        train_ds,\n",
        "        epochs=config['INITIAL_EPOCHS'],\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=val_ds_fit,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=callbacks_p1,\n",
        "        verbose=1,\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading best weights from Phase 1 checkpoint: {checkpoint_path}\")\n",
        "        final_model.load_weights(checkpoint_path)\n",
        "    else:\n",
        "        print(\"Warning: Checkpoint from Phase 1 not found.\")\n",
        "\n",
        "    fine_tune_layers_count = config.get('fine_tune_layers', 0)\n",
        "    hist_p2 = None\n",
        "\n",
        "    if fine_tune_layers_count > 0 and config['FINE_TUNE_EPOCHS'] > 0:\n",
        "        print(f\"\\n--- Setting up for Phase 2 Fine-tuning ---\")\n",
        "        print(f\"Unfreezing last {fine_tune_layers_count} layers of MobileNetV2 backbones.\")\n",
        "\n",
        "        for sn in config['spectrogram_signals']:\n",
        "            try:\n",
        "                base_model_layer = final_model.get_layer(f\"mobilenet_{sn}\")\n",
        "                base_model_layer.trainable = True\n",
        "\n",
        "                num_layers_in_backbone = len(base_model_layer.layers)\n",
        "                num_to_unfreeze = min(fine_tune_layers_count, num_layers_in_backbone)\n",
        "\n",
        "                print(f\"  For '{sn}': Unfreezing last {num_to_unfreeze} of {num_layers_in_backbone} layers.\")\n",
        "\n",
        "                for layer_idx, layer in enumerate(base_model_layer.layers[:-num_to_unfreeze]):\n",
        "                    layer.trainable = False\n",
        "\n",
        "                for layer_idx, layer in enumerate(base_model_layer.layers[-num_to_unfreeze:]):\n",
        "                    layer.trainable = True\n",
        "                print(f\"  Successfully set trainability for layers in '{sn}' backbone.\")\n",
        "\n",
        "            except ValueError:\n",
        "                print(f\"  Warning: MobileNetV2 backbone for '{sn}' not found. Cannot fine-tune.\")\n",
        "                continue\n",
        "\n",
        "        final_model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=config['learning_rate'] * config['fine_tune_lr_factor']),\n",
        "            loss=loss_fn,\n",
        "            metrics=metrics_list\n",
        "        )\n",
        "        print(f\"Model re-compiled for fine-tuning with LR: {config['learning_rate'] * config['fine_tune_lr_factor']:.2e}\")\n",
        "\n",
        "        callbacks_p2 = [\n",
        "            keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor=cb_monitor, mode='max', verbose=1),\n",
        "            keras.callbacks.EarlyStopping(monitor=cb_monitor_loss, patience=10, verbose=1, mode='min', restore_best_weights=True),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=cb_monitor_loss, factor=0.2, patience=5, verbose=1, min_lr=1e-7)\n",
        "        ]\n",
        "\n",
        "        start_epoch_p2 = hist_p1.epoch[-1] + 1 if hist_p1 and hist_p1.epoch else config['INITIAL_EPOCHS']\n",
        "        total_epochs_combined = config['INITIAL_EPOCHS'] + config['FINE_TUNE_EPOCHS']\n",
        "\n",
        "        print(f\"\\n--- Phase 2 Fine-tuning (up to {config['FINE_TUNE_EPOCHS']} additional epochs) ---\")\n",
        "        print(f\"Starting from epoch {start_epoch_p2}, total epochs planned: {total_epochs_combined}.\")\n",
        "\n",
        "        hist_p2 = final_model.fit(\n",
        "            train_ds,\n",
        "            epochs=total_epochs_combined,\n",
        "            initial_epoch=start_epoch_p2,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_data=val_ds_fit,\n",
        "            validation_steps=val_steps,\n",
        "            callbacks=callbacks_p2,\n",
        "            verbose=1,\n",
        "            class_weight=class_weights\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\nSkipping Phase 2 Fine-tuning.\")\n",
        "\n",
        "    best_model = final_model\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading overall best model from checkpoint: {checkpoint_path}\")\n",
        "        try:\n",
        "            best_model = keras.models.load_model(checkpoint_path, custom_objects={'BinaryFocalCrossentropy': loss_fn})\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading best model from checkpoint: {e}. Using model from end of training.\")\n",
        "            best_model = final_model\n",
        "    else:\n",
        "        print(\"Warning: Final best model checkpoint not found. Using model from end of training.\")\n",
        "\n",
        "    # Get Window Probabilities for ALL data splits (Train, Val, Test) for patient-level feature engineering\n",
        "    print(\"\\n--- Generating Window Probabilities for Patient-Level Feature Engineering ---\")\n",
        "    all_windows_df_with_probas = all_windows_df.copy() # Start with all metadata\n",
        "\n",
        "    # Create a dataset for all the windows to get predictions\n",
        "    all_windows_ds_for_pred = create_tf_dataset_windowed(all_windows_df_with_probas, config, current_bs, final_scaler, final_feature_cols, is_training=False, repeat_for_fit=False, data_base_path_val=config['training_data_dir'])\n",
        "    all_windows_steps = int(np.ceil(len(all_windows_df_with_probas) / current_bs)) if not all_windows_df_with_probas.empty and current_bs > 0 else 0\n",
        "\n",
        "    if all_windows_ds_for_pred and all_windows_steps > 0:\n",
        "        print(f\"Predicting probabilities for all {len(all_windows_df_with_probas)} windows...\")\n",
        "        all_probas = best_model.predict(all_windows_ds_for_pred, steps=all_windows_steps, verbose=1).flatten()\n",
        "        if len(all_probas) == len(all_windows_df_with_probas):\n",
        "            all_windows_df_with_probas['win_proba'] = all_probas\n",
        "        else:\n",
        "            print(f\"Warning: Mismatch in predicted probabilities ({len(all_probas)}) and total windows ({len(all_windows_df_with_probas)}). Patient-level features might be incorrect.\")\n",
        "            # Fallback: add a dummy column to prevent key errors, though results will be meaningless\n",
        "            all_windows_df_with_probas['win_proba'] = 0.0\n",
        "    else:\n",
        "        print(\"Warning: Could not create dataset for all window predictions. Patient-level features might be incorrect.\")\n",
        "        all_windows_df_with_probas['win_proba'] = 0.0 # Fallback\n",
        "\n",
        "    # Now filter this df for train, val, test sets\n",
        "    train_windows_for_patient_model_df = all_windows_df_with_probas[all_windows_df_with_probas['record_id'].isin(final_train_ids)].copy()\n",
        "    val_windows_for_patient_model_df = all_windows_df_with_probas[all_windows_df_with_probas['record_id'].isin(final_val_ids)].copy()\n",
        "    test_windows_for_patient_model_df = all_windows_df_with_probas[all_windows_df_with_probas['record_id'].isin(final_test_ids)].copy()\n",
        "\n",
        "\n",
        "    # --- Tune the Window-Level Decision Threshold ---\n",
        "    tuned_window_threshold = 0.5 # Default if tuning fails or val_df is empty\n",
        "    if best_model and not val_windows_for_patient_model_df.empty and 'win_proba' in val_windows_for_patient_model_df.columns:\n",
        "        print(\"\\n--- Tuning Window-Level Decision Threshold (Validation Set Windows) ---\")\n",
        "        val_win_probas_for_thresh_tuning = val_windows_for_patient_model_df['win_proba'].values\n",
        "        val_true_labels_for_thresh_tuning = val_windows_for_patient_model_df['label'].values\n",
        "\n",
        "        best_f1_win = -1.0\n",
        "        candidate_win_thresholds = np.arange(0.1, 0.91, 0.05)\n",
        "        print(f\"  Searching for optimal window decision threshold from candidates: {np.round(candidate_win_thresholds,2)}\")\n",
        "        for th in candidate_win_thresholds:\n",
        "            y_pred_win_val = (val_win_probas_for_thresh_tuning > th).astype(int)\n",
        "            f1 = f1_score(val_true_labels_for_thresh_tuning, y_pred_win_val, pos_label=1, average='binary', zero_division=0)\n",
        "            if f1 > best_f1_win:\n",
        "                best_f1_win = f1\n",
        "                tuned_window_threshold = th\n",
        "        print(f\"  Optimal Window-Level Decision Threshold (Val F1 for Event Class {best_f1_win:.4f}): {tuned_window_threshold:.2f}\")\n",
        "    else:\n",
        "        print(\"\\nSkipping window-level decision threshold tuning (no model/val_df/val_steps or win_proba missing). Using default 0.5.\")\n",
        "    config['tuned_window_threshold'] = tuned_window_threshold\n",
        "\n",
        "\n",
        "    # --- Patient-Level Modeling ---\n",
        "    secondary_patient_model = None\n",
        "    patient_feature_names = ['positive_window_ratio', 'mean_event_proba', 'std_proba', 'median_proba', 'p90_proba', 'num_event_segments']\n",
        "\n",
        "    if config['USE_SECONDARY_PATIENT_MODEL']:\n",
        "        print(\"\\n--- Training Secondary Patient-Level Model ---\")\n",
        "        # Engineer features for train, val\n",
        "        X_train_patient_df = engineer_patient_features(train_windows_for_patient_model_df, tuned_window_threshold, proba_col='win_proba')\n",
        "        X_val_patient_df = engineer_patient_features(val_windows_for_patient_model_df, tuned_window_threshold, proba_col='win_proba')\n",
        "\n",
        "        # Merge with the true patient labels\n",
        "        patient_true_labels_df_str_rec = patient_true_labels_df.copy()\n",
        "        patient_true_labels_df_str_rec['Record'] = patient_true_labels_df_str_rec['Record'].astype(str)\n",
        "\n",
        "        train_patient_data = pd.merge(X_train_patient_df, patient_true_labels_df_str_rec[['Record', config['patient_multiclass_target_col'], config['binary_target_col']]], left_on='record_id', right_on='Record', how='inner')\n",
        "        val_patient_data = pd.merge(X_val_patient_df, patient_true_labels_df_str_rec[['Record', config['patient_multiclass_target_col'], config['binary_target_col']]], left_on='record_id', right_on='Record', how='inner')\n",
        "\n",
        "        if not train_patient_data.empty and not val_patient_data.empty:\n",
        "            X_train_patient = train_patient_data[patient_feature_names].values\n",
        "            y_train_patient_multiclass = train_patient_data[config['patient_multiclass_target_col']].values\n",
        "\n",
        "            X_val_patient = val_patient_data[patient_feature_names].values\n",
        "            y_val_patient_multiclass = val_patient_data[config['patient_multiclass_target_col']].values\n",
        "\n",
        "            secondary_patient_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "            secondary_patient_model.fit(X_train_patient, y_train_patient_multiclass)\n",
        "            print(\"Secondary patient-level model (RandomForestClassifier) trained.\")\n",
        "\n",
        "            # Optional: Evaluate the secondary model on patient validation set\n",
        "            if len(X_val_patient) > 0:\n",
        "                y_pred_val_mc_secondary = secondary_patient_model.predict(X_val_patient)\n",
        "                val_f1_secondary = f1_score(y_val_patient_multiclass, y_pred_val_mc_secondary, average='macro', zero_division=0)\n",
        "                print(f\"  Secondary model F1 Macro on patient validation set: {val_f1_secondary:.4f}\")\n",
        "        else:\n",
        "            print(\"Warning: Not enough data to train/validate secondary patient model. Falling back to thresholding.\")\n",
        "            config['USE_SECONDARY_PATIENT_MODEL'] = False # Fallback\n",
        "\n",
        "    # If not using the secondary model or it failed, tune patient severity thresholds\n",
        "    tuned_patient_thresholds = list(config['PATIENT_SEVERITY_THRESHOLDS_ON_RATIO'])\n",
        "    if not config['USE_SECONDARY_PATIENT_MODEL']:\n",
        "        print(\"\\n--- Tuning Patient Severity Thresholds (Validation Set Windows - Thresholding Approach) ---\")\n",
        "        if not val_windows_for_patient_model_df.empty and 'win_proba' in val_windows_for_patient_model_df.columns:\n",
        "            val_windows_for_patient_model_df.loc[:, 'win_pred_binary_for_agg'] = (val_windows_for_patient_model_df['win_proba'] > tuned_window_threshold).astype(int)\n",
        "            val_patient_agg = val_windows_for_patient_model_df.groupby('record_id')['win_pred_binary_for_agg'].mean().reset_index().rename(columns={'win_pred_binary_for_agg':'positive_window_ratio'})\n",
        "            val_patient_agg['record_id'] = val_patient_agg['record_id'].astype(str)\n",
        "\n",
        "            patient_true_labels_df_str_rec = patient_true_labels_df.copy()\n",
        "            if 'Record' in patient_true_labels_df_str_rec.columns:\n",
        "                 patient_true_labels_df_str_rec['Record'] = patient_true_labels_df_str_rec['Record'].astype(str)\n",
        "            true_val_multi = patient_true_labels_df_str_rec[patient_true_labels_df_str_rec['Record'].isin(val_patient_agg['record_id'])][['Record', config['patient_multiclass_target_col']]].set_index('Record')\n",
        "            val_comp_df = pd.merge(val_patient_agg.set_index('record_id'), true_val_multi, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "            if not val_comp_df.empty and config['patient_multiclass_target_col'] in val_comp_df.columns:\n",
        "                y_true_val_mc = val_comp_df[config['patient_multiclass_target_col']]\n",
        "                y_score_val_ratio = val_comp_df['positive_window_ratio']\n",
        "                best_f1_val_mc = -1.0\n",
        "                candidate_th_sets = [\n",
        "                    [0.01, 0.03, 0.05], [0.02, 0.05, 0.10], [0.03, 0.08, 0.15],\n",
        "                    [0.04, 0.10, 0.20], [0.05, 0.12, 0.25], [0.05, 0.15, 0.30],\n",
        "                    [0.07, 0.20, 0.35], [0.10, 0.25, 0.40], [0.10, 0.30, 0.50],\n",
        "                    [0.15, 0.35, 0.55], [0.20, 0.40, 0.60]\n",
        "                ]\n",
        "                print(f\"  Searching for optimal patient severity thresholds from candidates: {candidate_th_sets}\")\n",
        "                for th_set in candidate_th_sets:\n",
        "                    conditions_mc = [(y_score_val_ratio < th_set[0]), (y_score_val_ratio >= th_set[0]) & (y_score_val_ratio < th_set[1]), (y_score_val_ratio >= th_set[1]) & (y_score_val_ratio < th_set[2]), (y_score_val_ratio >= th_set[2])]\n",
        "                    y_pred_val_mc = np.select(conditions_mc, [0,1,2,3], default=0)\n",
        "                    f1 = f1_score(y_true_val_mc, y_pred_val_mc, average='macro', zero_division=0)\n",
        "                    if f1 > best_f1_val_mc: best_f1_val_mc = f1; tuned_patient_thresholds = th_set\n",
        "                print(f\"  Optimal Patient Severity Thresholds (Val F1 Macro {best_f1_val_mc:.4f}): {tuned_patient_thresholds}\")\n",
        "            else: print(\"  Warning: Validation comparison DataFrame empty for patient threshold tuning.\")\n",
        "        else: print(\"  Warning: Validation window data missing for patient threshold tuning.\")\n",
        "    config['PATIENT_SEVERITY_THRESHOLDS_ON_RATIO'] = tuned_patient_thresholds\n",
        "\n",
        "    # --- Evaluate on the Test Set ---\n",
        "    if best_model and not test_windows_for_patient_model_df.empty:\n",
        "        print(\"\\n--- Evaluating Final Model (Test Set) ---\")\n",
        "\n",
        "        # Window-level evaluation (using the tuned_window_threshold)\n",
        "        if 'win_proba' in test_windows_for_patient_model_df.columns:\n",
        "            test_win_probas = test_windows_for_patient_model_df['win_proba'].values\n",
        "            test_true_labels_win = test_windows_for_patient_model_df['label'].values\n",
        "            current_tuned_window_threshold = config.get('tuned_window_threshold', 0.5)\n",
        "            test_pred_labels_win = (test_win_probas > current_tuned_window_threshold).astype(int)\n",
        "\n",
        "            print(\"\\n--- Window-Level Results (Test Set) ---\")\n",
        "            # Note: Re-evaluating loss, auc etc. on test_ds_eval would be more accurate than just using the probas\n",
        "            # For simplicity here, we'll focus on the metrics from binarized predictions\n",
        "            # test_win_results = best_model.evaluate(test_ds_eval, steps=test_steps_eval, verbose=0, return_dict=True) # If test_ds_eval is available\n",
        "            # for name, value in test_win_results.items(): print(f\"  {name}: {value:.4f}\")\n",
        "            print(f\"  (Using tuned window threshold: {current_tuned_window_threshold:.2f})\")\n",
        "            print(f\"  Accuracy: {accuracy_score(test_true_labels_win, test_pred_labels_win):.4f}\")\n",
        "            try: # AUC needs probabilities\n",
        "                print(f\"  AUC: {roc_auc_score(test_true_labels_win, test_win_probas):.4f}\")\n",
        "            except ValueError:\n",
        "                print(\"  AUC could not be calculated for window-level (possibly all labels are same).\")\n",
        "\n",
        "\n",
        "            cm_win_test = confusion_matrix(test_true_labels_win, test_pred_labels_win)\n",
        "            save_confusion_matrix_plot(cm_win_test, ['No Event', 'Event'], f'Window-Level CM (Test Set, Th={current_tuned_window_threshold:.2f})', os.path.join(FINAL_MODEL_RESULTS_DIR, \"cm_window_test.png\"))\n",
        "            print(f\"\\nWindow-Level Classification Report (Test Set, Threshold={current_tuned_window_threshold:.2f}):\")\n",
        "            print(classification_report(test_true_labels_win, test_pred_labels_win, target_names=['No Event', 'Event'], zero_division=0))\n",
        "        else:\n",
        "            print(\"Warning: 'win_proba' not found in test_windows_for_patient_model_df. Skipping window-level report.\")\n",
        "\n",
        "\n",
        "        # Patient-level evaluation\n",
        "        X_test_patient_df = engineer_patient_features(test_windows_for_patient_model_df, tuned_window_threshold, proba_col='win_proba')\n",
        "        patient_true_labels_df_str_rec_test = patient_true_labels_df.copy()\n",
        "        patient_true_labels_df_str_rec_test['Record'] = patient_true_labels_df_str_rec_test['Record'].astype(str)\n",
        "        test_patient_data_final = pd.merge(X_test_patient_df, patient_true_labels_df_str_rec_test[['Record', config['patient_multiclass_target_col'], config['binary_target_col']]], left_on='record_id', right_on='Record', how='inner')\n",
        "\n",
        "        if not test_patient_data_final.empty:\n",
        "            X_test_patient_final = test_patient_data_final[patient_feature_names].values\n",
        "            y_true_test_mc = test_patient_data_final[config['patient_multiclass_target_col']].values\n",
        "            y_true_test_binary = test_patient_data_final[config['binary_target_col']].values\n",
        "\n",
        "            y_pred_test_mc = None\n",
        "            prediction_method_info = \"\"\n",
        "\n",
        "            if config['USE_SECONDARY_PATIENT_MODEL'] and secondary_patient_model:\n",
        "                y_pred_test_mc = secondary_patient_model.predict(X_test_patient_final)\n",
        "                prediction_method_info = \"Secondary Model (RandomForest)\"\n",
        "            else: # Fallback to thresholding\n",
        "                # positive_window_ratio is already a column in X_test_patient_df (and thus in test_patient_data_final)\n",
        "                th_multi = config['PATIENT_SEVERITY_THRESHOLDS_ON_RATIO']\n",
        "                conditions_test_mc = [\n",
        "                    (test_patient_data_final['positive_window_ratio'] < th_multi[0]),\n",
        "                    (test_patient_data_final['positive_window_ratio'] >= th_multi[0]) & (test_patient_data_final['positive_window_ratio'] < th_multi[1]),\n",
        "                    (test_patient_data_final['positive_window_ratio'] >= th_multi[1]) & (test_patient_data_final['positive_window_ratio'] < th_multi[2]),\n",
        "                    (test_patient_data_final['positive_window_ratio'] >= th_multi[2])\n",
        "                ]\n",
        "                y_pred_test_mc = np.select(conditions_test_mc, [0,1,2,3], default=0)\n",
        "                prediction_method_info = f\"Thresholding (WinTh:{tuned_window_threshold:.2f}, PatientTh:{th_multi})\"\n",
        "\n",
        "            # Multi-class patient severity results\n",
        "            print(f\"\\n--- Patient-Level Multi-Class Severity Results (Test Set, Method: {prediction_method_info}) ---\")\n",
        "            print(f\"  Accuracy: {accuracy_score(y_true_test_mc, y_pred_test_mc):.4f}\")\n",
        "            cm_pat_test_mc = confusion_matrix(y_true_test_mc, y_pred_test_mc, labels=[0,1,2,3])\n",
        "            severity_names = ['Normal','Mild','Moderate','Severe']\n",
        "            save_confusion_matrix_plot(cm_pat_test_mc, severity_names, f'Patient Multi-Class Severity CM (Test Set - {prediction_method_info})', os.path.join(FINAL_MODEL_RESULTS_DIR, \"cm_patient_severity_multiclass_test.png\"))\n",
        "            print(\"\\nPatient-Level Multi-Class Severity Classification Report (Test Set):\")\n",
        "            print(classification_report(y_true_test_mc, y_pred_test_mc, target_names=severity_names, labels=[0,1,2,3], zero_division=0))\n",
        "\n",
        "            # Binary patient OSA results (derived from multi-class)\n",
        "            y_pred_test_binary = (y_pred_test_mc > 0).astype(int) # Normal (0) vs OSA (1,2,3 -> 1)\n",
        "            print(f\"\\n--- Patient-Level Binary OSA Results (Test Set, Derived from Multi-Class, Method: {prediction_method_info}) ---\")\n",
        "            print(f\"  Accuracy: {accuracy_score(y_true_test_binary, y_pred_test_binary):.4f}\")\n",
        "            cm_pat_test_binary = confusion_matrix(y_true_test_binary, y_pred_test_binary, labels=[0,1])\n",
        "            binary_names = ['No OSA','OSA']\n",
        "            save_confusion_matrix_plot(cm_pat_test_binary, binary_names, f'Patient Binary OSA CM (Test Set - {prediction_method_info})', os.path.join(FINAL_MODEL_RESULTS_DIR, \"cm_patient_binary_osa_test.png\"))\n",
        "            print(\"\\nPatient-Level Binary OSA Classification Report (Test Set):\")\n",
        "            print(classification_report(y_true_test_binary, y_pred_test_binary, target_names=binary_names, labels=[0,1], zero_division=0))\n",
        "\n",
        "            # Save detailed predictions\n",
        "            test_patient_data_final['pred_severity_multiclass'] = y_pred_test_mc\n",
        "            test_patient_data_final['pred_osa_binary'] = y_pred_test_binary\n",
        "            test_patient_data_final.to_csv(os.path.join(FINAL_MODEL_RESULTS_DIR, \"detailed_patient_predictions_test.csv\"), index=False)\n",
        "            print(f\"Saved detailed patient predictions to: {os.path.join(FINAL_MODEL_RESULTS_DIR, 'detailed_patient_predictions_test.csv')}\")\n",
        "        else:\n",
        "            print(\"  Warning: Test patient data empty or missing target. Skipping patient-level evaluation.\")\n",
        "    else:\n",
        "        print(\"\\nSkipping Test Set Evaluation (no model or test_df empty).\")\n",
        "\n",
        "    combined_history = {}\n",
        "    if hist_p1 and hist_p1.history:\n",
        "        for key, val in hist_p1.history.items():\n",
        "            combined_history[key] = list(val)\n",
        "\n",
        "    if hist_p2 and hist_p2.history:\n",
        "        for key, val in hist_p2.history.items():\n",
        "            if key in combined_history:\n",
        "                combined_history[key].extend(list(val))\n",
        "            else:\n",
        "                combined_history[key] = list(val)\n",
        "\n",
        "    if combined_history.get('loss'):\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(combined_history['loss'], label='Train Loss')\n",
        "        if 'val_loss' in combined_history:\n",
        "            plt.plot(combined_history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss During Training')\n",
        "        plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "        initial_epochs_count = len(hist_p1.history['loss']) if hist_p1 and hist_p1.history and 'loss' in hist_p1.history else 0\n",
        "        if hist_p2 and hist_p2.history and initial_epochs_count > 0 and initial_epochs_count < len(combined_history['loss']):\n",
        "             plt.axvline(x=initial_epochs_count -1 , color='gray', linestyle='--', label='Fine-tuning Start')\n",
        "        plt.legend(); plt.grid(True)\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        if 'accuracy' in combined_history: plt.plot(combined_history['accuracy'], label='Train Accuracy')\n",
        "        if 'val_accuracy' in combined_history: plt.plot(combined_history['val_accuracy'], label='Validation Accuracy')\n",
        "        if 'auc' in combined_history: plt.plot(combined_history['auc'], label='Train AUC')\n",
        "        if 'val_auc' in combined_history: plt.plot(combined_history['val_auc'], label='Validation AUC')\n",
        "        plt.title('Model Performance Metrics During Training')\n",
        "        plt.xlabel('Epoch'); plt.ylabel('Metric Value')\n",
        "        if hist_p2 and hist_p2.history and initial_epochs_count > 0 and initial_epochs_count < len(combined_history['loss']):\n",
        "             plt.axvline(x=initial_epochs_count -1, color='gray', linestyle='--', label='Fine-tuning Start')\n",
        "        plt.legend(); plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        history_plot_path = os.path.join(FINAL_MODEL_RESULTS_DIR, \"training_history_combined.png\")\n",
        "        plt.savefig(history_plot_path); plt.close()\n",
        "        print(f\"Saved combined training history plot to: {history_plot_path}\")\n",
        "    else:\n",
        "        print(\"No training history found to plot.\")\n",
        "\n",
        "    print(f\"\\nAttempting to clean up local data directory: {config['training_data_dir']}\")\n",
        "    if os.path.exists(config['training_data_dir']):\n",
        "        try:\n",
        "            shutil.rmtree(config['training_data_dir'])\n",
        "            print(f\"Successfully removed local data directory: {config['training_data_dir']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error removing local data directory {config['training_data_dir']}: {e}\")\n",
        "            print(\"You may need to remove it manually.\")\n",
        "\n",
        "    print(f\"\\n--- Sleep Apnea Analysis Script Finished ---\")\n",
        "    print(f\"Total execution time: {time.time() - main_start_time:.2f} seconds.\")\n",
        "    print(f\"All final model evaluation results and plots are saved in: {FINAL_MODEL_RESULTS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sPatuR7WTaFx",
        "outputId": "f48f5420-35b8-47bb-e9a6-ef6aa2e057dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring persistent base output directory exists: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model\n",
            "Preparing local base directory for processing/training: /content/processed_data_local_simplified_v2\n",
            "Output directories prepared.\n",
            "--- STAGE 1: Data Preprocessing and Windowing ---\n",
            "Created patient multi-class target 'AHI_Severity_MultiClass' using AHI thresholds: <5 (0), [5-15) (1), [15-30) (2), >=30 (3).\n",
            "Created patient binary target 'OSA_Severity_Binary' using AHI threshold: >=5 (1).\n",
            "Found 208 records in '/content/temp_data/selected_records208' that have corresponding entries in labels/demographics files.\n",
            "Attempting to load existing metadata from: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/all_processed_windows_metadata.csv\n",
            "Warning: Existing metadata does not cover all currently selected records. Reprocessing NPY files.\n",
            "Processing 208 records into windows (NPY files)...\n",
            "Using 4 processes for window generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1: Processing records (MP): 100%|| 208/208 [07:19<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Window processing (Stage 1) took: 440.22 seconds.\n",
            "Total windows generated: 386036\n",
            "Saved all processed windows metadata to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/all_processed_windows_metadata.csv\n",
            "--- Stage 1 Finished ---\n",
            "\n",
            "--- FINAL MODEL TRAINING AND EVALUATION STAGE ---\n",
            "Final Data Split (by Record IDs): Train: 134, Validation: 32, Test: 42\n",
            "Window counts: Train: 249563, Validation: 58568, Test: 77905\n",
            "Using tabular features for primary model: ['Age', 'Sex_encoded']\n",
            "Imputation values for tabular features: {'Age': 54.0, 'Sex_encoded': 1.0}\n",
            "StandardScaler fitted on training tabular data.\n",
            "Window-level class weights: {0: np.float64(0.5440634660411334), 1: np.float64(6.173634474569563)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-22d9e02df6f7>:668: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = keras.applications.MobileNetV2(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"windowed_multimodal_classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"windowed_multimodal_classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
              "\n",
              " raw_AIRFLOW_input (\u001b[38;5;33mInputLayer\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m1\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  -                         \n",
              "\n",
              " raw_C3-M2_input (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m1\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  -                         \n",
              "\n",
              " ts_conv1_AIRFLOW (\u001b[38;5;33mConv1D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m32\u001b[0m)                            \u001b[38;5;34m544\u001b[0m  raw_AIRFLOW_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " ts_conv1_C3-M2 (\u001b[38;5;33mConv1D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m32\u001b[0m)                            \u001b[38;5;34m544\u001b[0m  raw_C3-M2_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " spec_SaO2_input (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  -                         \n",
              "\n",
              " spec_ECG_input (\u001b[38;5;33mInputLayer\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  -                         \n",
              "\n",
              " ts_bn1_AIRFLOW                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m32\u001b[0m)                            \u001b[38;5;34m128\u001b[0m  ts_conv1_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                          \n",
              "\n",
              " ts_bn1_C3-M2 (\u001b[38;5;33mBatchNormalization\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6000\u001b[0m, \u001b[38;5;34m32\u001b[0m)                            \u001b[38;5;34m128\u001b[0m  ts_conv1_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " true_divide_2 (\u001b[38;5;33mTrueDivide\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  spec_SaO2_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " true_divide_3 (\u001b[38;5;33mTrueDivide\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  spec_ECG_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " ts_pool1_AIRFLOW (\u001b[38;5;33mMaxPooling1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m32\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  ts_bn1_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " ts_pool1_C3-M2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m32\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  ts_bn1_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " tabular_input (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                                     \u001b[38;5;34m0\u001b[0m  -                         \n",
              "\n",
              " subtract_2 (\u001b[38;5;33mSubtract\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  true_divide_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " subtract_3 (\u001b[38;5;33mSubtract\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  true_divide_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " ts_conv2_AIRFLOW (\u001b[38;5;33mConv1D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m32,832\u001b[0m  ts_pool1_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " ts_conv2_C3-M2 (\u001b[38;5;33mConv1D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m32,832\u001b[0m  ts_pool1_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                   \u001b[38;5;34m96\u001b[0m  tabular_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mobilenet_SaO2 (\u001b[38;5;33mFunctional\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                          \u001b[38;5;34m2,257,984\u001b[0m  subtract_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " mobilenet_ECG (\u001b[38;5;33mFunctional\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                          \u001b[38;5;34m2,257,984\u001b[0m  subtract_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " ts_bn2_AIRFLOW                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m64\u001b[0m)                            \u001b[38;5;34m256\u001b[0m  ts_conv2_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                          \n",
              "\n",
              " ts_bn2_C3-M2 (\u001b[38;5;33mBatchNormalization\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m64\u001b[0m)                            \u001b[38;5;34m256\u001b[0m  ts_conv2_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " batch_normalization_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                  \u001b[38;5;34m128\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                          \n",
              "\n",
              " dropout_spec_SaO2 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                                  \u001b[38;5;34m0\u001b[0m  mobilenet_SaO2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_spec_ECG (\u001b[38;5;33mDropout\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                                  \u001b[38;5;34m0\u001b[0m  mobilenet_ECG[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " ts_features_AIRFLOW                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                                    \u001b[38;5;34m0\u001b[0m  ts_bn2_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                                                                                          \n",
              "\n",
              " ts_features_C3-M2                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                                    \u001b[38;5;34m0\u001b[0m  ts_bn2_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                                                                                          \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                    \u001b[38;5;34m0\u001b[0m  batch_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
              "\n",
              " spec_features_SaO2 (\u001b[38;5;33mDense\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                               \u001b[38;5;34m81,984\u001b[0m  dropout_spec_SaO2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " spec_features_ECG (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                               \u001b[38;5;34m81,984\u001b[0m  dropout_spec_ECG[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " ts_dense_AIRFLOW (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                                \u001b[38;5;34m4,160\u001b[0m  ts_features_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " ts_dense_C3-M2 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                                \u001b[38;5;34m4,160\u001b[0m  ts_features_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " tabular_features (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                                \u001b[38;5;34m2,112\u001b[0m  dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " fused_features (\u001b[38;5;33mConcatenate\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                                   \u001b[38;5;34m0\u001b[0m  spec_features_SaO2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], \n",
              "                                                                                     spec_features_ECG[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n",
              "                                                                                     ts_dense_AIRFLOW[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n",
              "                                                                                     ts_dense_C3-M2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                                                     tabular_features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                              \u001b[38;5;34m41,088\u001b[0m  fused_features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " batch_normalization_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                                 \u001b[38;5;34m512\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                          \n",
              "\n",
              " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
              "\n",
              " output (\u001b[38;5;33mDense\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                                   \u001b[38;5;34m129\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                      </span><span style=\"font-weight: bold\"> Output Shape                 </span><span style=\"font-weight: bold\">           Param # </span><span style=\"font-weight: bold\"> Connected to              </span>\n",
              "\n",
              " raw_AIRFLOW_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
              "\n",
              " raw_C3-M2_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
              "\n",
              " ts_conv1_AIRFLOW (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>  raw_AIRFLOW_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " ts_conv1_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>  raw_C3-M2_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " spec_SaO2_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
              "\n",
              " spec_ECG_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
              "\n",
              " ts_bn1_AIRFLOW                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  ts_conv1_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                          \n",
              "\n",
              " ts_bn1_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  ts_conv1_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " true_divide_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  spec_SaO2_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " true_divide_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  spec_ECG_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " ts_pool1_AIRFLOW (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  ts_bn1_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " ts_pool1_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  ts_bn1_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " tabular_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
              "\n",
              " subtract_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  true_divide_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " subtract_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  true_divide_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " ts_conv2_AIRFLOW (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span>  ts_pool1_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " ts_conv2_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span>  ts_pool1_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>  tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mobilenet_SaO2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span>  subtract_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " mobilenet_ECG (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span>  subtract_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " ts_bn2_AIRFLOW                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  ts_conv2_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                          \n",
              "\n",
              " ts_bn2_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  ts_conv2_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " batch_normalization_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                  <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                          \n",
              "\n",
              " dropout_spec_SaO2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mobilenet_SaO2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_spec_ECG (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mobilenet_ECG[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " ts_features_AIRFLOW                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  ts_bn2_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                                                                                          \n",
              "\n",
              " ts_features_C3-M2                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  ts_bn2_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                                                                                          \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
              "\n",
              " spec_features_SaO2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span>  dropout_spec_SaO2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " spec_features_ECG (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span>  dropout_spec_ECG[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " ts_dense_AIRFLOW (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  ts_features_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " ts_dense_C3-M2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  ts_features_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " tabular_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span>  dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " fused_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  spec_features_SaO2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], \n",
              "                                                                                     spec_features_ECG[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n",
              "                                                                                     ts_dense_AIRFLOW[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n",
              "                                                                                     ts_dense_C3-M2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                                                     tabular_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span>  fused_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " batch_normalization_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                                 <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                          \n",
              "\n",
              " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
              "\n",
              " output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,799,841\u001b[0m (18.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,799,841</span> (18.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m283,169\u001b[0m (1.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">283,169</span> (1.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,516,672\u001b[0m (17.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,516,672</span> (17.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Phase 1 Training (15 epochs) ---\n",
            "Monitoring 'val_auc' for ModelCheckpoint and 'val_loss' for EarlyStopping/ReduceLROnPlateau.\n",
            "Epoch 1/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5929 - auc: 0.7501 - loss: 0.3449 - precision: 0.4129 - recall: 0.8554\n",
            "Epoch 1: val_auc improved from -inf to 0.84160, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1632s\u001b[0m 186ms/step - accuracy: 0.5929 - auc: 0.7501 - loss: 0.3449 - precision: 0.4129 - recall: 0.8554 - val_accuracy: 0.4428 - val_auc: 0.8416 - val_loss: 0.1558 - val_precision: 0.1154 - val_recall: 0.9523 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.5880 - auc: 0.8363 - loss: 0.1287 - precision: 0.4176 - recall: 0.9406\n",
            "Epoch 2: val_auc improved from 0.84160 to 0.84617, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1384s\u001b[0m 177ms/step - accuracy: 0.5880 - auc: 0.8363 - loss: 0.1287 - precision: 0.4176 - recall: 0.9406 - val_accuracy: 0.5157 - val_auc: 0.8462 - val_loss: 0.1244 - val_precision: 0.1280 - val_recall: 0.9264 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.6146 - auc: 0.8876 - loss: 0.0950 - precision: 0.4350 - recall: 0.9606\n",
            "Epoch 3: val_auc improved from 0.84617 to 0.85160, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 180ms/step - accuracy: 0.6146 - auc: 0.8876 - loss: 0.0950 - precision: 0.4350 - recall: 0.9606 - val_accuracy: 0.7271 - val_auc: 0.8516 - val_loss: 0.0810 - val_precision: 0.1938 - val_recall: 0.8222 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6597 - auc: 0.9065 - loss: 0.0870 - precision: 0.4669 - recall: 0.9662\n",
            "Epoch 4: val_auc improved from 0.85160 to 0.86336, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1402s\u001b[0m 180ms/step - accuracy: 0.6597 - auc: 0.9065 - loss: 0.0870 - precision: 0.4669 - recall: 0.9662 - val_accuracy: 0.7840 - val_auc: 0.8634 - val_loss: 0.0665 - val_precision: 0.2270 - val_recall: 0.7683 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.6851 - auc: 0.9137 - loss: 0.0833 - precision: 0.4883 - recall: 0.9653\n",
            "Epoch 5: val_auc improved from 0.86336 to 0.86569, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1408s\u001b[0m 181ms/step - accuracy: 0.6851 - auc: 0.9137 - loss: 0.0833 - precision: 0.4883 - recall: 0.9653 - val_accuracy: 0.8220 - val_auc: 0.8657 - val_loss: 0.0543 - val_precision: 0.2603 - val_recall: 0.7312 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6993 - auc: 0.9188 - loss: 0.0806 - precision: 0.4995 - recall: 0.9677\n",
            "Epoch 6: val_auc improved from 0.86569 to 0.86645, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1397s\u001b[0m 179ms/step - accuracy: 0.6993 - auc: 0.9188 - loss: 0.0806 - precision: 0.4995 - recall: 0.9677 - val_accuracy: 0.8304 - val_auc: 0.8665 - val_loss: 0.0536 - val_precision: 0.2680 - val_recall: 0.7139 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7146 - auc: 0.9241 - loss: 0.0780 - precision: 0.5130 - recall: 0.9697\n",
            "Epoch 7: val_auc did not improve from 0.86645\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 178ms/step - accuracy: 0.7146 - auc: 0.9241 - loss: 0.0780 - precision: 0.5130 - recall: 0.9697 - val_accuracy: 0.8458 - val_auc: 0.8647 - val_loss: 0.0516 - val_precision: 0.2854 - val_recall: 0.6869 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7234 - auc: 0.9280 - loss: 0.0758 - precision: 0.5203 - recall: 0.9687\n",
            "Epoch 8: val_auc improved from 0.86645 to 0.87341, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1383s\u001b[0m 177ms/step - accuracy: 0.7234 - auc: 0.9280 - loss: 0.0758 - precision: 0.5203 - recall: 0.9687 - val_accuracy: 0.8474 - val_auc: 0.8734 - val_loss: 0.0505 - val_precision: 0.2896 - val_recall: 0.6963 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7327 - auc: 0.9309 - loss: 0.0738 - precision: 0.5301 - recall: 0.9725\n",
            "Epoch 9: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1371s\u001b[0m 176ms/step - accuracy: 0.7327 - auc: 0.9309 - loss: 0.0738 - precision: 0.5301 - recall: 0.9725 - val_accuracy: 0.8469 - val_auc: 0.8659 - val_loss: 0.0489 - val_precision: 0.2873 - val_recall: 0.6880 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7438 - auc: 0.9340 - loss: 0.0720 - precision: 0.5403 - recall: 0.9737\n",
            "Epoch 10: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1378s\u001b[0m 177ms/step - accuracy: 0.7438 - auc: 0.9340 - loss: 0.0720 - precision: 0.5403 - recall: 0.9737 - val_accuracy: 0.7870 - val_auc: 0.8685 - val_loss: 0.0637 - val_precision: 0.2310 - val_recall: 0.7765 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7520 - auc: 0.9362 - loss: 0.0707 - precision: 0.5492 - recall: 0.9748\n",
            "Epoch 11: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1383s\u001b[0m 177ms/step - accuracy: 0.7520 - auc: 0.9362 - loss: 0.0707 - precision: 0.5492 - recall: 0.9748 - val_accuracy: 0.8579 - val_auc: 0.8677 - val_loss: 0.0471 - val_precision: 0.3050 - val_recall: 0.6828 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7616 - auc: 0.9390 - loss: 0.0686 - precision: 0.5581 - recall: 0.9761\n",
            "Epoch 12: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1372s\u001b[0m 176ms/step - accuracy: 0.7616 - auc: 0.9390 - loss: 0.0686 - precision: 0.5581 - recall: 0.9761 - val_accuracy: 0.8432 - val_auc: 0.8634 - val_loss: 0.0512 - val_precision: 0.2826 - val_recall: 0.6936 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7722 - auc: 0.9427 - loss: 0.0663 - precision: 0.5698 - recall: 0.9783\n",
            "Epoch 13: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1404s\u001b[0m 180ms/step - accuracy: 0.7722 - auc: 0.9427 - loss: 0.0663 - precision: 0.5698 - recall: 0.9783 - val_accuracy: 0.8522 - val_auc: 0.8715 - val_loss: 0.0479 - val_precision: 0.2975 - val_recall: 0.6970 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7763 - auc: 0.9437 - loss: 0.0658 - precision: 0.5760 - recall: 0.9786\n",
            "Epoch 14: val_auc did not improve from 0.87341\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 180ms/step - accuracy: 0.7763 - auc: 0.9437 - loss: 0.0658 - precision: 0.5760 - recall: 0.9786 - val_accuracy: 0.8279 - val_auc: 0.8666 - val_loss: 0.0535 - val_precision: 0.2662 - val_recall: 0.7224 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7900 - auc: 0.9488 - loss: 0.0622 - precision: 0.5909 - recall: 0.9812\n",
            "Epoch 15: val_auc did not improve from 0.87341\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1402s\u001b[0m 180ms/step - accuracy: 0.7900 - auc: 0.9488 - loss: 0.0622 - precision: 0.5909 - recall: 0.9812 - val_accuracy: 0.8606 - val_auc: 0.8604 - val_loss: 0.0470 - val_precision: 0.3048 - val_recall: 0.6549 - learning_rate: 2.0000e-05\n",
            "Loading best weights from Phase 1 checkpoint: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\n",
            "--- Setting up for Phase 2 Fine-tuning ---\n",
            "Unfreezing last 15 layers of MobileNetV2 backbones.\n",
            "  For 'SaO2': Unfreezing last 15 of 155 layers.\n",
            "  Successfully set trainability for layers in 'SaO2' backbone.\n",
            "  For 'ECG': Unfreezing last 15 of 155 layers.\n",
            "  Successfully set trainability for layers in 'ECG' backbone.\n",
            "Model re-compiled for fine-tuning with LR: 1.00e-05\n",
            "\n",
            "--- Phase 2 Fine-tuning (up to 15 additional epochs) ---\n",
            "Starting from epoch 15, total epochs planned: 30.\n",
            "Epoch 16/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7258 - auc: 0.9181 - loss: 0.0831 - precision: 0.4826 - recall: 0.9076\n",
            "Epoch 16: val_auc improved from -inf to 0.87208, saving model to /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1622s\u001b[0m 185ms/step - accuracy: 0.7258 - auc: 0.9181 - loss: 0.0831 - precision: 0.4826 - recall: 0.9076 - val_accuracy: 0.8777 - val_auc: 0.8721 - val_loss: 0.0430 - val_precision: 0.3384 - val_recall: 0.6414 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7422 - auc: 0.9373 - loss: 0.0706 - precision: 0.5398 - recall: 0.9765\n",
            "Epoch 17: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1388s\u001b[0m 178ms/step - accuracy: 0.7422 - auc: 0.9373 - loss: 0.0706 - precision: 0.5398 - recall: 0.9765 - val_accuracy: 0.8768 - val_auc: 0.8720 - val_loss: 0.0435 - val_precision: 0.3374 - val_recall: 0.6481 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7613 - auc: 0.9433 - loss: 0.0672 - precision: 0.5591 - recall: 0.9789\n",
            "Epoch 18: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1410s\u001b[0m 181ms/step - accuracy: 0.7613 - auc: 0.9433 - loss: 0.0672 - precision: 0.5591 - recall: 0.9789 - val_accuracy: 0.9088 - val_auc: 0.8653 - val_loss: 0.0363 - val_precision: 0.4157 - val_recall: 0.4998 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7668 - auc: 0.9447 - loss: 0.0663 - precision: 0.5640 - recall: 0.9785\n",
            "Epoch 19: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1401s\u001b[0m 180ms/step - accuracy: 0.7668 - auc: 0.9447 - loss: 0.0663 - precision: 0.5640 - recall: 0.9785 - val_accuracy: 0.8879 - val_auc: 0.8631 - val_loss: 0.0404 - val_precision: 0.3540 - val_recall: 0.5799 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7790 - auc: 0.9477 - loss: 0.0642 - precision: 0.5781 - recall: 0.9796\n",
            "Epoch 20: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1409s\u001b[0m 181ms/step - accuracy: 0.7790 - auc: 0.9477 - loss: 0.0642 - precision: 0.5781 - recall: 0.9796 - val_accuracy: 0.8989 - val_auc: 0.8636 - val_loss: 0.0382 - val_precision: 0.3814 - val_recall: 0.5358 - learning_rate: 1.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7862 - auc: 0.9501 - loss: 0.0626 - precision: 0.5854 - recall: 0.9812\n",
            "Epoch 21: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1410s\u001b[0m 181ms/step - accuracy: 0.7862 - auc: 0.9501 - loss: 0.0626 - precision: 0.5854 - recall: 0.9812 - val_accuracy: 0.8844 - val_auc: 0.8574 - val_loss: 0.0422 - val_precision: 0.3429 - val_recall: 0.5720 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7941 - auc: 0.9529 - loss: 0.0608 - precision: 0.5948 - recall: 0.9824\n",
            "Epoch 22: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1433s\u001b[0m 184ms/step - accuracy: 0.7941 - auc: 0.9529 - loss: 0.0608 - precision: 0.5948 - recall: 0.9824 - val_accuracy: 0.8962 - val_auc: 0.8604 - val_loss: 0.0398 - val_precision: 0.3707 - val_recall: 0.5288 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8005 - auc: 0.9540 - loss: 0.0596 - precision: 0.6015 - recall: 0.9834\n",
            "Epoch 23: val_auc did not improve from 0.87208\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1445s\u001b[0m 185ms/step - accuracy: 0.8005 - auc: 0.9540 - loss: 0.0596 - precision: 0.6015 - recall: 0.9834 - val_accuracy: 0.9070 - val_auc: 0.8565 - val_loss: 0.0388 - val_precision: 0.4035 - val_recall: 0.4721 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8069 - auc: 0.9561 - loss: 0.0582 - precision: 0.6098 - recall: 0.9838\n",
            "Epoch 24: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1484s\u001b[0m 190ms/step - accuracy: 0.8069 - auc: 0.9561 - loss: 0.0582 - precision: 0.6098 - recall: 0.9838 - val_accuracy: 0.9117 - val_auc: 0.8538 - val_loss: 0.0392 - val_precision: 0.4212 - val_recall: 0.4403 - learning_rate: 2.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8122 - auc: 0.9572 - loss: 0.0573 - precision: 0.6174 - recall: 0.9841\n",
            "Epoch 25: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1497s\u001b[0m 192ms/step - accuracy: 0.8122 - auc: 0.9572 - loss: 0.0573 - precision: 0.6174 - recall: 0.9841 - val_accuracy: 0.9106 - val_auc: 0.8514 - val_loss: 0.0400 - val_precision: 0.4143 - val_recall: 0.4331 - learning_rate: 2.0000e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8166 - auc: 0.9592 - loss: 0.0562 - precision: 0.6220 - recall: 0.9851\n",
            "Epoch 26: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1502s\u001b[0m 193ms/step - accuracy: 0.8166 - auc: 0.9592 - loss: 0.0562 - precision: 0.6220 - recall: 0.9851 - val_accuracy: 0.9056 - val_auc: 0.8619 - val_loss: 0.0395 - val_precision: 0.3993 - val_recall: 0.4863 - learning_rate: 2.0000e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8159 - auc: 0.9588 - loss: 0.0565 - precision: 0.6216 - recall: 0.9849\n",
            "Epoch 27: val_auc did not improve from 0.87208\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1506s\u001b[0m 193ms/step - accuracy: 0.8159 - auc: 0.9588 - loss: 0.0565 - precision: 0.6216 - recall: 0.9849 - val_accuracy: 0.9149 - val_auc: 0.8597 - val_loss: 0.0388 - val_precision: 0.4372 - val_recall: 0.4248 - learning_rate: 2.0000e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8192 - auc: 0.9595 - loss: 0.0558 - precision: 0.6261 - recall: 0.9846\n",
            "Epoch 28: val_auc did not improve from 0.87208\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
            "\u001b[1m7799/7799\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1513s\u001b[0m 194ms/step - accuracy: 0.8192 - auc: 0.9595 - loss: 0.0558 - precision: 0.6261 - recall: 0.9846 - val_accuracy: 0.9079 - val_auc: 0.8581 - val_loss: 0.0397 - val_precision: 0.4090 - val_recall: 0.4824 - learning_rate: 2.0000e-06\n",
            "Epoch 28: early stopping\n",
            "Restoring model weights from the end of the best epoch: 18.\n",
            "Loading overall best model from checkpoint: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/best_final_model.keras\n",
            "\n",
            "--- Generating Window Probabilities for Patient-Level Feature Engineering ---\n",
            "Predicting probabilities for all 386036 windows...\n",
            "\u001b[1m12064/12064\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1347s\u001b[0m 111ms/step\n",
            "\n",
            "--- Tuning Window-Level Decision Threshold (Validation Set Windows) ---\n",
            "  Searching for optimal window decision threshold from candidates: [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
            " 0.8  0.85 0.9 ]\n",
            "  Optimal Window-Level Decision Threshold (Val F1 for Event Class 0.4717): 0.55\n",
            "\n",
            "--- Training Secondary Patient-Level Model ---\n",
            "Secondary patient-level model (RandomForestClassifier) trained.\n",
            "  Secondary model F1 Macro on patient validation set: 0.5286\n",
            "\n",
            "--- Evaluating Final Model (Test Set) ---\n",
            "\n",
            "--- Window-Level Results (Test Set) ---\n",
            "  (Using tuned window threshold: 0.55)\n",
            "  Accuracy: 0.9120\n",
            "  AUC: 0.8821\n",
            "Saved confusion matrix plot to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/cm_window_test.png\n",
            "\n",
            "Window-Level Classification Report (Test Set, Threshold=0.55):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Event       0.96      0.94      0.95     71768\n",
            "       Event       0.45      0.57      0.51      6137\n",
            "\n",
            "    accuracy                           0.91     77905\n",
            "   macro avg       0.71      0.76      0.73     77905\n",
            "weighted avg       0.92      0.91      0.92     77905\n",
            "\n",
            "\n",
            "--- Patient-Level Multi-Class Severity Results (Test Set, Method: Secondary Model (RandomForest)) ---\n",
            "  Accuracy: 0.7381\n",
            "Saved confusion matrix plot to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/cm_patient_severity_multiclass_test.png\n",
            "\n",
            "Patient-Level Multi-Class Severity Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.77      0.87      0.82        23\n",
            "        Mild       0.58      0.54      0.56        13\n",
            "    Moderate       1.00      0.75      0.86         4\n",
            "      Severe       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.74        42\n",
            "   macro avg       0.84      0.66      0.73        42\n",
            "weighted avg       0.74      0.74      0.73        42\n",
            "\n",
            "\n",
            "--- Patient-Level Binary OSA Results (Test Set, Derived from Multi-Class, Method: Secondary Model (RandomForest)) ---\n",
            "  Accuracy: 0.7857\n",
            "Saved confusion matrix plot to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/cm_patient_binary_osa_test.png\n",
            "\n",
            "Patient-Level Binary OSA Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      No OSA       0.77      0.87      0.82        23\n",
            "         OSA       0.81      0.68      0.74        19\n",
            "\n",
            "    accuracy                           0.79        42\n",
            "   macro avg       0.79      0.78      0.78        42\n",
            "weighted avg       0.79      0.79      0.78        42\n",
            "\n",
            "Saved detailed patient predictions to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/detailed_patient_predictions_test.csv\n",
            "Saved combined training history plot to: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results/training_history_combined.png\n",
            "\n",
            "Attempting to clean up local data directory: /content/processed_data_local_simplified_v2\n",
            "Successfully removed local data directory: /content/processed_data_local_simplified_v2\n",
            "\n",
            "--- Sleep Apnea Analysis Script Finished ---\n",
            "Total execution time: 42020.08 seconds.\n",
            "All final model evaluation results and plots are saved in: /content/drive/MyDrive/AI_Sleep_Apnea/output/sleep_apnea_analysis_simplified_208_v2_secondary_model/final_model_evaluation_results\n"
          ]
        }
      ]
    }
  ]
}